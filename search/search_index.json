{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Table of Contents","text":""},{"location":"#vision-in-code-mastering-convolutional-neural-networks-for-real-world-image-modeling","title":"Vision in Code: Mastering Convolutional Neural Networks for Real-World Image Modeling","text":""},{"location":"#a-practical-guide-to-cnn-implementation-with-pytorch-and-tensorflow","title":"A Practical Guide to CNN Implementation with PyTorch and TensorFlow","text":""},{"location":"#contents","title":"Contents","text":""},{"location":"#preface","title":"\ud83d\udcd6 Preface","text":"<ul> <li> <p>Why This Book Exists</p> </li> <li> <p>Who Should Read This</p> </li> <li> <p>From Pixels to Convolutions: How This Book Was Born</p> </li> <li> <p>What You\u2019ll Learn (and What You Won\u2019t)</p> </li> <li> <p>How to Read This Book (Even if You\u2019re Just Starting Out)</p> </li> </ul>"},{"location":"#part-i-foundations-of-image-tensors-and-preprocessing","title":"Part I \u2013 Foundations of Image Tensors and Preprocessing","text":"<p>\u00a0\u00a0\u00a0\u00a0 Chapter 1: How a Neural Network Sees an Image</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01.1 What is an image (JPEG, PNG, etc.) in memory?</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01.2 From pixel data \u2192 NumPy array \u2192 Tensor</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01.3 RGB channels, 8-bit scale, float conversion</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01.4 [H, W, C] vs [C, H, W] \u2014 framework differences explained</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01.5 Why model input shape matters</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01.6 Visual walkthrough of image-to-input pipeline</p> <p>\u00a0\u00a0\u00a0\u00a0 Chapter 2: What is a Tensor (in Code and in Mind)?</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a02.1 Tensor shapes and memory layout</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a02.2 Dimensionality intuition</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a02.3 PyTorch: torch.tensor, .permute(), .view(), .reshape()</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a02.4 TensorFlow: tf.Tensor, .reshape(), .transpose(), broadcasting</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a02.5 Visual walkthroughs of shape manipulations</p> <p>\u00a0\u00a0\u00a0\u00a0 Chapter 3: From Pixels to Model Input</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a03.1 Full image input pipeline:</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a03.2 RGB loading \u2192 float32 conversion \u2192 normalization</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a03.3 Resizing and reshaping to expected input size</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a03.4 Batch dimension handling: unsqueeze() vs expand_dims()</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a03.5 Feeding tensors into Dense or Conv2D layers</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a03.6 Debugging mismatched shapes</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a03.7 Framework comparison of entire image \u2192 tensor \u2192 model flow</p>"},{"location":"#part-ii-preprocessing-and-input-pipelines","title":"Part II \u2013 Preprocessing and Input Pipelines","text":"<p>\u00a0\u00a0\u00a0\u00a0 Chapter 4: Standard Image Preprocessing</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a04.1 Resize, Normalize, Augment</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a04.2 Mean-std normalization vs 0\u20131 scaling</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a04.3 Format mismatches and their impact on accuracy</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a04.4 PIL vs OpenCV vs tf.image</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a04.5 Visualizing preprocessing effects</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a04.6 Matching preprocessing between training and inference</p> <p>\u00a0\u00a0\u00a0\u00a0 Chapter 5: Preprocessing for Pretrained Models</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a05.1 Matching pretrained model expectations: MobileNetV2, EfficientNet, ResNet, etc.</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a05.2 transforms.Normalize vs tf.keras.applications.*.preprocess_input()</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a05.3 PyTorch: torchvision.models</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a05.4 TensorFlow: keras.applications</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a05.5 Inference vs training preprocessing pitfalls</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a05.6 Side-by-side code snippets for each model</p> <p>\u00a0\u00a0\u00a0\u00a0 Chapter 6: Image Datasets: Getting Data Into the Network</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a06.1 Folder structure conventions</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a06.2 PyTorch: Dataset, DataLoader, ImageFolder, transforms</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a06.3 TensorFlow: tf.data.Dataset, image_dataset_from_directory()</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a06.4 Label mapping, batching, and shuffling</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a06.5 Visualizing batches from both frameworks</p> <p>\u00a0\u00a0\u00a0\u00a0 Chapter 7: Data Augmentation Techniques (Expanded)</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a07.1 Common augmentations: RandomCrop, ColorJitter, Cutout</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a07.2 Advanced augmentations: Mixup, CutMix (optional)</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a07.3 PyTorch: torchvision.transforms</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a07.4 TensorFlow: tf.image, Keras preprocessing layers</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a07.5 Before/after visualization of augmentation effects</p>"},{"location":"#part-iii-cnn-architectures-and-concepts","title":"Part III \u2013 CNN Architectures and Concepts","text":"<p>\u00a0\u00a0\u00a0\u00a0 Chapter 8: Understanding CNN Layers</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a08.1 Kernels, filters, channels, strides, padding</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a08.2 Pooling (Max, Average), ReLU, BatchNorm</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a08.3 PyTorch: nn.Conv2d, nn.MaxPool2d, nn.BatchNorm2d, etc.</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a08.4 TensorFlow: Conv2D, MaxPooling2D, BatchNormalization, etc.</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a08.5 Conceptual breakdown + syntax comparison</p> <p>\u00a0\u00a0\u00a0\u00a0 Chapter 9: The CNN Vocabulary (Terms Demystified)</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a09.1 Key terms: kernel, convolution, stride, padding</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a09.2 Input/output channels, feature maps</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a09.3 Convolutional layer vs residual block</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a09.4 Layer variants: ReflectionPad2d, InstanceNorm2d, AdaptiveAvgPool2d</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a09.5 Visual and code-based examples</p> <p>\u00a0\u00a0\u00a0\u00a0 Chapter 10: Writing the forward() / call() Function</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a010.1 PyTorch: forward(), self.features, self.classifier</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a010.2 TensorFlow: call(), subclassing Model</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a010.3 Layer-by-layer flow visualized</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a010.4 Common mistakes in model building</p> <p>\u00a0\u00a0\u00a0\u00a0 Chapter 11: Model Summary and Parameter Inspection</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a011.1 PyTorch: model.parameters(), summary(), state_dict()</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a011.2 TensorFlow: .summary(), get_weights(), trainable_variables</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a011.3 How to freeze/unfreeze layers for fine-tuning</p> <p>\u00a0\u00a0\u00a0\u00a0 Chapter 12: Building Your First CNN: Patterns and Pitfalls</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a012.1 Simple architectures: LeNet-style, Mini-VGG</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a012.2 Choosing filter sizes, kernel shapes, stride</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a012.3 Stacking layers: when and why</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a012.4 Common design mistakes (too few filters, wrong input shape, etc.)</p>"},{"location":"#part-iv-training-and-fine-tuning","title":"Part IV \u2013 Training and Fine-Tuning","text":"<p>\u00a0\u00a0\u00a0\u00a0 Chapter 13: Loss Functions and Optimizers</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a013.1 PyTorch: loss_fn(), .backward(), optimizer.step()</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a013.2 TensorFlow: GradientTape, optimizer.apply_gradients()</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a013.3 Common losses: CrossEntropy</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a013.4 Optimizers: SGD, Adam</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a013.5 Visualizing gradient flow</p> <p>\u00a0\u00a0\u00a0\u00a0 Chapter 14: Training Loop Mechanics</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a014.1 PyTorch: full training loop with train_loader</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a014.2 TensorFlow: model.fit() vs custom training loop</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a014.3 Logging loss and metrics</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a014.4 Checkpoint saving, early stopping</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a014.5 Adding visuals for debugging and learning</p> <p>\u00a0\u00a0\u00a0\u00a0 Chapter 15: Training Strategies and Fine-Tuning Pretrained CNNs</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a015.1 When to Fine-Tune vs Freeze</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a015.2 Adapting Pretrained Models</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a015.3 Regularization Techniques</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a015.4 Training Strategies for Generalization</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a015.5 Recognizing Overfitting and Underfitting</p>"},{"location":"#part-v-inference-evaluation-and-visual-debugging","title":"Part V \u2013 Inference, Evaluation, and Visual Debugging","text":"<p>\u00a0\u00a0\u00a0\u00a0 Chapter 16: Train vs Eval Mode</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a016.1 PyTorch: model.train(), model.eval(), no_grad()</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a016.2 TensorFlow: training=True/False</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a016.3 Dropout and BatchNorm behavior</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a016.4 Impact of mode on inference</p> <p>\u00a0\u00a0\u00a0\u00a0 Chapter 17: Visualizing Feature Maps and Filters</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a017.1 Getting intermediate layer outputs</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a017.2 PyTorch: forward hooks, manual slicing</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a017.3 ensorFlow: defining sub-models</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a017.4 Visualizing what the model is focusing on</p>"},{"location":"#part-vi-deployment-ready-insights","title":"Part VI \u2013 Deployment-Ready Insights","text":"<p>\u00a0\u00a0\u00a0\u00a0 Chapter 18: Inference Pipeline Design</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a018.1 Keeping preprocessing consistent (train vs inference)</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a018.2 Reusable preprocess functions</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a018.3 Input validation, test-time augmentation</p> <p>\u00a0\u00a0\u00a0\u00a0 Chapter 19: Common Errors and How to Debug Them</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a019.1 Model always predicts one class? Check normalization</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a019.2 Input shape mismatch? Check dataloader</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a019.3 Nothing\u2019s working? Try a single image pipeline</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a019.4 Debugging checklist for CNN-based models</p>"},{"location":"#appendices","title":"Appendices","text":"<p>A. PyTorch vs TensorFlow Cheatsheet</p> <p>B. Troubleshooting Image Model Failures</p> <p>C. Glossary of Key Terms</p> <p>D. Pretrained Model Reference Table (with links)</p> <p>E. Sample Projects and Mini-Exercises per Chapter</p>"},{"location":"#chapter-format","title":"Chapter Format","text":"<p>Each chapter ends with:</p> <ul> <li> <p>Conceptual Breakdown</p> </li> <li> <p>PyTorch Implementation</p> </li> <li> <p>TensorFlow Implementation</p> </li> <li> <p>Framework Comparison Table</p> </li> <li> <p>Use Case or Mini-Exercise</p> </li> </ul>"},{"location":"PartIII_overview/","title":"Part III \u2013 CNN Architectures and Concepts","text":"<p>\u201cThe data flows in\u2014but what happens next? This is where filters activate, features emerge, and vision becomes understanding.\u201d</p>"},{"location":"PartIII_overview/#why-this-part-matters","title":"Why This Part Matters","text":"<p>You\u2019ve resized, normalized, and batched your image data.</p> <p>But how does a neural network actually see a dog\u2019s ear, detect a road sign, or recognize a handwritten digit?</p> <p>It happens through the architecture: the convolutional layers, activations, pooling operations, and more that together form a trainable perception system. This part walks you through that internal machinery.</p> <p>By the end of Part III, you\u2019ll be able to:</p> <ul> <li>Build CNN architectures from scratch</li> <li>Understand how each layer transforms the input</li> <li>Spot design patterns used in popular models</li> <li>Interpret what each layer contributes to learning</li> </ul>"},{"location":"PartIII_overview/#what-youll-master-in-this-part","title":"What You\u2019ll Master in This Part","text":"<ul> <li>The anatomy of CNN layers\u2014what they do and how to use them</li> <li>Common terms like kernel, stride, padding, feature map, and filter</li> <li>The logic behind forward passes and <code>forward()</code>/<code>call()</code> methods</li> <li>How to inspect a model\u2019s structure and parameter count</li> <li>CNN design patterns used in real-world architectures like LeNet and Mini-VGG</li> </ul> <p>Each chapter dives deeper into the building blocks, visualizes internal flows, and compares implementation across PyTorch and TensorFlow\u2014so you gain full control over how your models are structured and trained.</p>"},{"location":"PartIII_overview/#chapter-breakdown","title":"Chapter Breakdown","text":"Chapter Title What You\u2019ll Learn 8 Understanding CNN Layers Kernels, strides, padding, pooling, activations, and normalization layers 9 The CNN Vocabulary (Terms Demystified) What filters, channels, and feature maps really mean\u2014visually and in code 10 Writing the forward() / call() Function Model subclassing, layer flow, common mistakes, debug-friendly architectures 11 Model Summary and Parameter Inspection Counting parameters, freezing/unfreezing layers, weight inspection 12 Building Your First CNN: Patterns and Pitfalls Hands-on with LeNet, Mini-VGG, filter size strategies, design mistakes to avoid"},{"location":"PartIII_overview/#key-ideas-that-tie-this-part-together","title":"Key Ideas That Tie This Part Together","text":"<ol> <li> <p>Layers aren\u2019t black boxes\u2014they\u2019re geometric transformers.</p> </li> <li> <p>Each convolution shrinks or expands your input in predictable ways. Knowing how lets you design with intention.</p> </li> <li> <p>Your model is a blueprint.</p> </li> <li> <p>Whether you\u2019re subclassing in PyTorch or TensorFlow, you need to know how tensors flow from layer to layer\u2014and why.</p> </li> <li> <p>Architecture \u2260 Accuracy\u2014but it enables it.</p> </li> <li> <p>Fancy models don\u2019t guarantee results, but clear, modular, well-planned architectures make success reproducible.</p> </li> </ol>"},{"location":"PartIII_overview/#what-makes-this-part-unique","title":"What Makes This Part Unique","text":"<p>Most books or tutorials show a pre-written CNN, but they rarely:</p> <ul> <li>Walk you through layer-by-layer decisions</li> <li>Explain what happens visually inside each operation</li> <li>Compare <code>Conv2d</code> vs <code>Conv2D</code> with actual shape flows</li> <li>Show what happens when you pass an image into your <code>forward()</code> method</li> </ul> <p>This part does.</p>"},{"location":"PartIII_overview/#learning-goal-for-this-part","title":"Learning Goal for This Part","text":"<p>By the end of Part III, you should be able to:</p> <ul> <li>Construct custom CNNs for new tasks</li> <li>Modify and fine-tune pretrained architectures</li> <li>Understand internal layer behavior and diagnose structural bugs</li> <li>Experiment confidently with kernel sizes, channel depths, and activation strategies</li> </ul>"},{"location":"PartII_overview/","title":"Part II \u2013 Preprocessing and Input Pipelines","text":"<p>\u201cA neural network is only as good as the data you feed it. Preprocessing isn't just a step\u2014it's a commitment to model performance.\u201d</p>"},{"location":"PartII_overview/#why-this-part-matters","title":"Why This Part Matters","text":"<p>You've learned how to convert a single image into a clean tensor. But in real-world deep learning, you're not dealing with just one image\u2014you\u2019re working with thousands or even millions. Your pipeline must:</p> <ul> <li> <p>Handle batches efficiently</p> </li> <li> <p>Preprocess images consistently</p> </li> <li> <p>Apply augmentations during training</p> </li> <li> <p>Load and shuffle datasets at scale</p> </li> </ul> <p>Poor preprocessing leads to:</p> <ul> <li> <p>Models that memorize backgrounds instead of objects</p> </li> <li> <p>Validation accuracy that lags way behind training accuracy</p> </li> <li> <p>Deployment failures due to mismatched input formats</p> </li> </ul> <p>This part teaches you to build scalable, error-proof pipelines that transform raw datasets into high-quality model inputs.</p>"},{"location":"PartII_overview/#what-youll-master-in-this-part","title":"What You\u2019ll Master in This Part","text":"<ul> <li> <p>Resize, crop, normalize, and augment images using the right tools</p> </li> <li> <p>Understand the subtle difference between training and inference preprocessing</p> </li> <li> <p>Load folders of images into batches using <code>Dataset</code>, <code>DataLoader</code>, or <code>image_dataset_from_directory</code></p> </li> <li> <p>Apply data augmentation techniques to improve generalization</p> </li> </ul> <p>And most importantly, you\u2019ll see how these decisions affect your model\u2019s learning.</p>"},{"location":"PartII_overview/#chapter-breakdown","title":"Chapter Breakdown","text":"Chapter Title What You\u2019ll Learn 4 Standard Image Preprocessing Resize, normalize, augment; difference between mean-std and 0\u20131 scaling; effects on training and inference 5 Preprocessing for Pretrained Models How to match image formats to model expectations (e.g., ResNet, MobileNet), using transforms.Normalize vs keras.applications.preprocess_input 6 Image Datasets: Getting Data Into the Network Loading entire datasets from folders, batching, shuffling, label mapping 7 Data Augmentation Techniques (Expanded) RandomCrop, ColorJitter, Cutout, Mixup, CutMix, and how to implement them in PyTorch and TensorFlow"},{"location":"PartII_overview/#key-ideas-that-tie-this-part-together","title":"Key Ideas That Tie This Part Together","text":"<ol> <li> <p>Reproducibility begins at preprocessing.</p> <ul> <li>If your training and inference pipelines differ, your model may perform well in notebooks but fail in production.</li> </ul> </li> <li> <p>Augmentation is not optional\u2014it\u2019s critical.</p> <ul> <li>Especially in small datasets, it\u2019s what makes your CNN generalize.</li> </ul> </li> <li> <p>Data pipelines should be modular, debuggable, and scalable.</p> <ul> <li>Whether using torchvision.datasets.ImageFolder or tf.data.Dataset, this part shows how to do it right.</li> </ul> </li> </ol>"},{"location":"PartII_overview/#lets-build-smarter-input-pipelines","title":"Let\u2019s Build Smarter Input Pipelines","text":"<p>This part is where your models start to learn better. With cleaner data, smarter augmentation, and consistent preprocessing, you\u2019ll start to see training curves that actually make sense\u2014and validation metrics that finally catch up.</p>"},{"location":"PartIV_overview/","title":"Part IV \u2013 Training and Fine-Tuning","text":"<p>\u201cYou\u2019ve built the network. Now comes the real challenge: teaching it to learn\u2014effectively, efficiently, and without forgetting what matters.\u201d</p>"},{"location":"PartIV_overview/#why-this-part-matters","title":"Why This Part Matters","text":"<p>A perfectly constructed CNN is useless if it doesn't learn well. Training isn't just running <code>.fit()</code> or looping over epochs. It's about:</p> <ul> <li>Choosing the right loss function</li> <li>Controlling how weights update via optimizers</li> <li>Avoiding overfitting with regularization</li> <li>Implementing custom training loops</li> <li>Knowing when to stop, what to freeze, and what to adapt</li> </ul> <p>This part shows you how to go beyond \u201ctraining for accuracy\u201d and start training for robust generalization.</p>"},{"location":"PartIV_overview/#what-youll-master-in-this-part","title":"What You\u2019ll Master in This Part","text":"<ul> <li>Understand how backpropagation, loss gradients, and weight updates work</li> <li>Master PyTorch and TensorFlow training loop mechanics</li> <li>Apply strategies like early stopping, learning rate scheduling, and layer freezing</li> <li>Fine-tune pretrained CNNs on your own datasets</li> <li>Identify signs of underfitting, overfitting, or data imbalance</li> </ul> <p>This part is where you gain control over how learning happens.</p>"},{"location":"PartIV_overview/#chapter-breakdown","title":"Chapter Breakdown","text":"Chapter Title What You\u2019ll Learn 13 Loss Functions and Optimizers How networks learn via gradient descent, key loss functions (CrossEntropy), and optimizers like SGD and Adam 14 Training Loop Mechanics How to build full training loops in PyTorch and TensorFlow, including epoch tracking, metrics, and checkpoints 15 Training Strategies and Fine-Tuning Pretrained CNNs How to freeze/unfreeze layers, adapt models to new datasets, and use regularization effectively 16 Train vs Eval Mode Why dropout and batch norm behave differently in train vs eval mode, and how to handle inference correctly 17 Visualizing Feature Maps and Filters How to peek inside your CNN during and after training using hooks or submodels to visualize what it \"sees\""},{"location":"PartIV_overview/#key-ideas-that-tie-this-part-together","title":"\ud83d\udca1 Key Ideas That Tie This Part Together","text":"<ol> <li> <p>Training is not automatic\u2014it\u2019s a guided process.</p> </li> <li> <p>Model quality depends on data quality, learning rate, loss signals, and architecture alignment.</p> </li> <li> <p>Overfitting is easy. Generalization is art.</p> </li> <li> <p>Preventing a model from memorizing training data is harder than most beginners realize. Augmentation and regularization are essential.</p> </li> <li> <p>Pretrained models need care.</p> </li> <li> <p>You can\u2019t just throw new data at them\u2014layers must be frozen/unfrozen with purpose, and inputs must be matched.</p> </li> </ol>"},{"location":"PartIV_overview/#what-youll-be-able-to-do-after-this-part","title":"What You\u2019ll Be Able To Do After This Part","text":"<ul> <li>Train CNNs from scratch with effective optimizers</li> <li>Implement reproducible training pipelines with logging and saving</li> <li>Debug training failures by inspecting loss curves and gradients</li> <li>Fine-tune ImageNet models for your custom use cases (e.g., cats vs dogs, X-rays vs MRI)</li> <li>Visualize how a model \u201cactivates\u201d for different parts of an image</li> </ul>"},{"location":"PartI_overview/","title":"Part I \u2013 Foundations of Image Tensors and Preprocessing","text":"<p>\u201cBefore we learn to classify, segment, or detect\u2026 we must learn to feed the model. And that starts with understanding how neural networks perceive images.\u201d</p>"},{"location":"PartI_overview/#seeing-through-the-models-eyes","title":"\ud83d\udc41\ufe0f Seeing Through the Model\u2019s Eyes","text":"<p>Convolutional Neural Networks are brilliant\u2014but only when you talk to them in their native language: tensors. If you feed a CNN the wrong shape, wrong scale, or wrong format, it won\u2019t complain immediately\u2014it will just fail silently, learning the wrong things or nothing at all.</p> <p>That\u2019s why Part I is all about foundations. Here, we zoom in on the seemingly \u201csimple\u201d steps that trip up most beginners (and even experienced practitioners when switching frameworks).</p> <p>These first three chapters serve one mission:</p> <p>To teach you how to guide a raw image into a form that a CNN can understand, process, and learn from\u2014without surprises.</p>"},{"location":"PartI_overview/#what-youll-master-in-this-part","title":"What You\u2019ll Master in This Part","text":"<p>What is an image from a neural network\u2019s perspective (beyond what you see in a photo viewer)</p> <p>Tensor fundamentals\u2014what shapes mean, how memory layout works, and how to reshape, permute, and batch like a pro</p> <p>The full input pipeline\u2014from disk to tensor to Conv2D-ready data, both in PyTorch and TensorFlow</p> <p>How to debug common issues like: \u201cshape mismatch,\u201d \u201cexpected 3 channels,\u201d or \u201cmodel outputs garbage\u201d</p>"},{"location":"PartI_overview/#chapter-breakdown","title":"Chapter Breakdown","text":"Chapter Title What You\u2019ll Learn 1 How a Neural Network Sees an Image JPEG vs raw pixel data, RGB channels, tensor layout differences, visual walkthrough of image \u2192 input 2 What is a Tensor (in Code and in Mind)? Tensor shapes, dimensionality, reshaping, broadcasting, permute vs transpose 3 From Pixels to Model Input Full preprocessing pipeline, float32 conversion, normalization, batching, feeding into Conv2D layers"},{"location":"PartI_overview/#why-this-part-matters","title":"Why This Part Matters","text":"<p>If your CNN is performing poorly, don\u2019t blame the model yet. Nine times out of ten, it\u2019s not your architecture\u2014it\u2019s your input.</p> <p>This part will teach you how to:</p> <ul> <li> <p>Make preprocessing repeatable and reliable</p> </li> <li> <p>Build a mental model of how CNNs consume images</p> </li> <li> <p>Speak fluently in tensor shapes and formats across frameworks</p> </li> </ul> <p>By the time you finish Part I, you won\u2019t just be \u201cloading images\u201d\u2014you\u2019ll be preparing them for intelligent perception.</p>"},{"location":"PartVI_overview/","title":"Part VI \u2013 Deployment-Ready Insights","text":"<p>\u201cA model is only as good as the pipeline it lives in.\u201d</p>"},{"location":"PartVI_overview/#why-this-part-matters","title":"Why This Part Matters","text":"<p>Training an accurate model is only half the battle. The real challenge often starts after training:</p> <ul> <li>Deploying the model to real-world environments</li> <li>Feeding it consistent and valid input</li> <li>Debugging unexpected behaviors</li> <li>Making sure it performs reliably across devices, datasets, and users</li> </ul> <p>Even small mistakes\u2014like using different image normalization values\u2014can completely break predictions.</p> <p>This part teaches you:</p> <ul> <li>How to build robust inference pipelines</li> <li>How to identify and fix common CNN errors</li> <li>How to prevent deployment disasters with checklists and defensive coding</li> </ul>"},{"location":"PartVI_overview/#chapter-breakdown","title":"Chapter Breakdown","text":"Chapter Title What You\u2019ll Learn 18 Inference Pipeline Design How to build robust, consistent input \u2192 output systems 19 Common Errors and How to Debug Them Learn the most common CNN bugs and how to solve them fast"},{"location":"PartVI_overview/#what-youll-master-in-this-part","title":"What You\u2019ll Master in This Part","text":"<ul> <li>Consistent preprocessing at inference time</li> <li>Building reusable pipelines across training, validation, and deployment</li> <li>Writing robust input handlers to avoid shape or type mismatches</li> <li>Understanding test-time augmentation (TTA) for better performance</li> <li> <p>Diagnosing silent model failures like:</p> </li> <li> <p>Always predicting the same class</p> </li> <li>Failing due to shape mismatches</li> <li>Dropping accuracy after deployment</li> </ul>"},{"location":"PartVI_overview/#tools-youll-be-using","title":"Tools You\u2019ll Be Using","text":"Tool / Concept Purpose <code>transforms.Normalize()</code> / <code>keras.applications.preprocess_input()</code> Match train-time normalization during inference TorchScript / TensorFlow SavedModel Export for deployment Reusable preprocessing functions DRY inference code <code>torchvision.transforms.Compose()</code> Modular preprocessing chain Shape checkers + asserts Prevent bad input Matplotlib debug plots Visualize mismatches and filter failure"},{"location":"PartVI_overview/#real-problems-solved-here","title":"Real Problems Solved Here","text":"Real-World Issue Solution You\u2019ll Learn Model performs great in training but fails live Normalize input same as during training Predictions don\u2019t make sense at inference Check model.eval(), use no_grad Model always outputs one class Review dataset balance, check activation function Shape mismatch crashes Add shape logging and assertions Deployment runs slow Strip gradients, batch inputs, optimize model export"},{"location":"PartVI_overview/#after-this-part-youll-be-able-to","title":"After This Part, You'll Be Able To:","text":"<ul> <li>Confidently deploy CNNs into apps, APIs, or devices</li> <li> <p>Build pipelines that handle:</p> </li> <li> <p>Preprocessing</p> </li> <li>Inference</li> <li>Output formatting</li> <li>Debug failure modes fast\u2014even when the model is a black box</li> <li>Use a checklist approach to verify CNN behavior across environments</li> </ul>"},{"location":"PartVI_overview/#from-research-to-production","title":"From Research to Production","text":"<p>Most deep learning models never make it past the training notebook. Why?</p> <p>Because deployment involves:</p> <ul> <li>Inconsistent input formats</li> <li>Subtle preprocessing bugs</li> <li>Misuse of model mode (<code>train</code> vs <code>eval</code>)</li> <li>Missing validations or sanity checks</li> </ul> <p>In this part, you\u2019ll learn the hidden work of making CNNs ready for the real world.</p>"},{"location":"PartV_overview/","title":"Part V \u2013 Inference, Evaluation, and Visual Debugging","text":"<p>\u201cOnce the model is trained, the real-world test begins. Inference is the moment your model meets reality.\u201d</p>"},{"location":"PartV_overview/#why-this-part-matters","title":"Why This Part Matters","text":"<p>Training a CNN isn\u2019t the final goal. What really matters is what happens after training:</p> <ul> <li>Does it make correct predictions on unseen data?</li> <li>Does it behave consistently across platforms?</li> <li>Can you debug it when it fails?</li> </ul> <p>Most real-world issues occur after training:</p> <ul> <li>Input mismatch during inference</li> <li>Dropout or BatchNorm behaving incorrectly</li> <li>Unexplainable results that need visualization</li> </ul> <p>This part of the book equips you to evaluate models confidently and debug them visually.</p>"},{"location":"PartV_overview/#what-youll-master-in-this-part","title":"What You\u2019ll Master in This Part","text":"<ul> <li>Understand and use the correct training/evaluation modes</li> <li>Handle Dropout and BatchNorm properly during inference</li> <li>Perform accurate predictions in real-time systems</li> <li>Visualize feature maps and filters to interpret what the model is learning</li> <li>Build intuition for CNN internals using visual tools</li> </ul>"},{"location":"PartV_overview/#chapter-breakdown","title":"Chapter Breakdown","text":"Chapter Title What You\u2019ll Learn 16 Train vs Eval Mode Understand mode switching, dropout behavior, and inference consistency 17 Visualizing Feature Maps and Filters Peek inside CNNs to see what filters are activating and when"},{"location":"PartV_overview/#key-problems-this-part-solves","title":"Key Problems This Part Solves","text":"Problem Solution You'll Learn Model behaves differently at inference Use <code>model.eval()</code> or <code>training=False</code> properly Accuracy drops on deployment Check normalization, dropout, batchnorm behavior Can't explain model predictions Visualize filters and activations Confusion between logits and probabilities Confirm mode, output, and softmax usage Debugging performance bottlenecks Test inference-only mode with no gradients"},{"location":"PartV_overview/#tools-youll-be-using","title":"Tools You\u2019ll Be Using","text":"Tool Description <code>model.eval()</code> / <code>.train()</code> Control dropout/batchnorm in PyTorch <code>training=True/False</code> Control training/inference in TensorFlow <code>torch.no_grad()</code> Disable gradients for inference Keras <code>Model(inputs, outputs)</code> Create sub-models for visualization PyTorch forward hooks Inspect layer outputs during forward pass <code>matplotlib</code>, <code>seaborn</code> Plot activation maps and filters"},{"location":"PartV_overview/#what-youll-be-able-to-do-after-this-part","title":"What You\u2019ll Be Able To Do After This Part","text":"<ul> <li>Switch between training and inference with complete confidence</li> <li>Ensure consistent behavior between training and deployment environments</li> <li>Analyze intermediate activations in CNNs</li> <li>Debug failing models visually</li> <li>Understand how convolution filters \u201clight up\u201d for different image regions</li> </ul>"},{"location":"PartV_overview/#real-world-relevance","title":"Real-World Relevance","text":"<p>In production, bugs often stem not from your training pipeline\u2014but from incorrect inference behavior or misunderstood activations.</p> <p>You\u2019ll encounter scenarios like:</p> <ul> <li>A model doing well in training but failing in deployment due to <code>Dropout</code> still being active</li> <li>A medical image classifier that seems to focus on text labels in X-rays\u2014visible through feature maps</li> <li>A classifier always choosing one class\u2014fixable only after visualizing activation saturation</li> </ul> <p>This part helps you trust, verify, and explain what your CNN is doing under the hood.</p>"},{"location":"Preface/","title":"&nbsp; Preface","text":""},{"location":"Preface/#why-this-book-exists","title":"Why This Book Exists","text":"<p>The age of pixels is long behind us\u2014now we interpret vision through patterns, activations, and filters. This book was born from one guiding principle: to make deep learning in computer vision understandable, practical, and deeply empowering.</p> <p>I\u2019ve spent countless hours not just building convolutional neural networks, but debugging them, reshaping tensors, and wondering why a model stubbornly predicts a single class. I wrote this book for every builder who has ever stared at a blurry output or an incomprehensible shape mismatch and thought: I wish there were a clear, visual, code-first explanation for this.</p> <p>This is not a book about academic theory. It\u2019s a builder\u2019s guide\u2014every concept is grounded in the practical realities of implementation using PyTorch and TensorFlow. Whether you\u2019re classifying dog breeds, recognizing traffic signs, or building drone-based vision systems, the goal is to make CNNs feel like second nature in your hands.</p>"},{"location":"Preface/#who-should-read-this","title":"Who Should Read This","text":"<p>This book is for:</p> <ul> <li>Engineers and ML developers who want to go beyond tutorials and understand what\u2019s actually happening under the hood.</li> <li>Graduate students and thesis builders looking to integrate CNNs into real-world systems\u2014whether it\u2019s for research, deployment, or startup MVPs.</li> <li>Tinkerers and self-learners who are done with theoretical detours and want to build fast, fail forward, and learn through code.</li> </ul> <p>A solid grasp of Python and NumPy will help, but this book walks you through everything else\u2014especially where things usually go wrong.</p>"},{"location":"Preface/#from-pixels-to-convolutions-how-this-book-was-born","title":"From Pixels to Convolutions: How This Book Was Born","text":"<p>Like most things in deep learning, this started as a curiosity\u2014how does a neural network see an image?</p> <p>The first time I traced an image from disk through RGB channels to tensor format and finally into a convolutional layer, I realized: this journey is invisible to most learners. Yet it\u2019s foundational.</p> <p>So I wrote down every shape transformation, every gotcha, every hidden framework behavior that wasn\u2019t explained well in blog posts. Then I organized those insights into chapters. What you\u2019re holding now is the result of that journey\u2014a distilled, visual-first walkthrough of how images become models.</p>"},{"location":"Preface/#what-youll-learn-and-what-you-wont","title":"What You\u2019ll Learn (and What You Won\u2019t)","text":"<p>You will learn:</p> <ul> <li>How to go from image files to normalized tensors ready for inference.</li> <li>How CNN layers operate, how to design small networks, and how to train them from scratch.</li> <li>How pretrained models expect inputs\u2014and how to avoid mismatches.</li> <li>How to visualize what a model \"sees\" and where it focuses.</li> <li>How to debug input shape mismatches, overfitting, data imbalance, and training collapse.</li> </ul> <p>You will not find:</p> <ul> <li>Abstract mathematical derivations.</li> <li>Discussions on GANs, diffusion models, or transformers (unless CNN-related).</li> <li>Generic tutorials that avoid framework specifics.</li> </ul> <p>This is a hands-on, image-in \u2192 tensor-out \u2192 model-learn book.</p>"},{"location":"Preface/#how-to-read-this-book-even-if-youre-just-starting-out","title":"How to Read This Book (Even if You\u2019re Just Starting Out)","text":"<p>Each chapter ends with:</p> <ul> <li>Conceptual Breakdown: Understand what\u2019s happening and why.</li> <li>PyTorch Implementation: Learn by doing it in PyTorch.</li> <li>TensorFlow Implementation: Learn the TensorFlow way.</li> <li>Framework Comparison Table: Spot the differences instantly.</li> <li>Mini-Exercise: Try it on your own with guidance.</li> </ul> <p>You don\u2019t have to read every line of code. But if you want to build CNNs in production\u2014or debug them\u2014you will want to understand why that one line of <code>permute()</code> or <code>expand_dims()</code> matters.</p>"},{"location":"appendices/","title":"Appendices","text":""},{"location":"appendices/#appendix-a-pytorch-vs-tensorflow-cheatsheet","title":"Appendix A. PyTorch vs TensorFlow Cheatsheet","text":"<p>This quick reference is designed to help you switch between frameworks with confidence. It highlights the syntax, structure, and behavioral differences that you need to remember during development.</p> Task PyTorch TensorFlow Import <code>import torch</code> <code>import tensorflow as tf</code> Tensor Creation <code>torch.tensor([[1., 2.], [3., 4.]])</code> <code>tf.constant([[1., 2.], [3., 4.]])</code> Shape Format <code>[C, H, W]</code> <code>[H, W, C]</code> Reshape <code>tensor.view(-1, 3, 32, 32)</code> <code>tf.reshape(tensor, [-1, 32, 32, 3])</code> Permute/Transpose <code>x.permute(0, 2, 3, 1)</code> <code>tf.transpose(x, [0, 2, 3, 1])</code> Device Handling <code>x.to(\"cuda\")</code> Automatically uses GPU if available Model Definition Subclass <code>nn.Module</code> Subclass <code>tf.keras.Model</code> Forward Pass <code>def forward(self, x):</code> <code>def call(self, inputs):</code> Loss Functions <code>nn.CrossEntropyLoss()</code> <code>tf.keras.losses.CategoricalCrossentropy()</code> Optimizers <code>torch.optim.Adam()</code> <code>tf.keras.optimizers.Adam()</code> Training Mode <code>model.train()</code> / <code>model.eval()</code> <code>training=True</code> / <code>False</code> Disable Gradients <code>with torch.no_grad():</code> <code>@tf.function</code> or <code>with tf.GradientTape():</code> Checkpointing <code>torch.save(model.state_dict())</code> <code>model.save_weights()</code> <p>Use this table anytime you're translating code or debugging between frameworks.</p>"},{"location":"appendices/#appendix-b-troubleshooting-image-model-failures","title":"Appendix B. Troubleshooting Image Model Failures","text":"<p>Training an image classifier but it keeps failing? Here\u2019s a diagnostic checklist.</p>"},{"location":"appendices/#symptom-model-predicts-only-one-class","title":"\u26a0\ufe0f Symptom: Model Predicts Only One Class","text":"<ul> <li>Check normalization: are pixel values normalized properly (0\u20131 or mean-std)?</li> <li>Inspect label imbalance\u2014does one class dominate the training data?</li> <li>Try a confusion matrix to confirm.</li> </ul>"},{"location":"appendices/#symptom-shape-mismatch-errors","title":"\u26a0\ufe0f Symptom: Shape Mismatch Errors","text":"<ul> <li>Inspect input shape: is <code>[C, H, W]</code> (PyTorch) or <code>[H, W, C]</code> (TensorFlow)?</li> <li>Add <code>.unsqueeze(0)</code> or <code>expand_dims()</code> if batch dim is missing.</li> <li>Is the model expecting 3 channels? Grayscale images may only have 1.</li> </ul>"},{"location":"appendices/#symptom-loss-not-decreasing","title":"\u26a0\ufe0f Symptom: Loss Not Decreasing","text":"<ul> <li>Use <code>model.train()</code> (PyTorch) or <code>training=True</code> (TensorFlow).</li> <li>Check learning rate: too high causes instability, too low slows learning.</li> <li>Are labels correctly encoded (integers for CrossEntropy, one-hot for categorical)?</li> </ul>"},{"location":"appendices/#symptom-good-train-accuracy-poor-val-accuracy","title":"\u26a0\ufe0f Symptom: Good Train Accuracy, Poor Val Accuracy","text":"<ul> <li>Overfitting? Add dropout or L2 regularization.</li> <li>Add more data or augmentations.</li> <li>Match preprocessing between training and inference.</li> </ul>"},{"location":"appendices/#tip-always-test-with-a-single-image-first","title":"Tip: Always Test with a Single Image First","text":"<pre><code># PyTorch single image test\nmodel.eval()\nwith torch.no_grad():\n    out = model(image.unsqueeze(0).to(device))\n</code></pre>"},{"location":"appendices/#appendix-c-glossary-of-key-terms","title":"Appendix C. Glossary of Key Terms","text":"<p>A quick dictionary of the most important terms in CNN-based vision.</p> Term Definition Tensor A multi-dimensional array (e.g., 4D tensor for image batches). Convolution A sliding window operation to extract spatial patterns. Kernel / Filter The small matrix (e.g., 3\u00d73) used in convolutions. Stride Step size the filter moves each time. Padding Adding borders to preserve spatial size after convolution. Channel Color or feature dimension (e.g., RGB = 3 channels). Batch Size Number of images processed in one forward/backward pass. Pooling Downsampling layer (MaxPool, AvgPool) to reduce dimensions. Activation Non-linearity applied after each layer (ReLU, Sigmoid). Feature Map Output of a convolutional layer representing features. Fine-Tuning Adapting a pretrained model on a new dataset. Freeze Layers Prevent layers from updating during training. Transfer Learning Reusing a trained model\u2019s knowledge in a new task."},{"location":"appendices/#appendix-d-pretrained-model-reference-table","title":"Appendix D. Pretrained Model Reference Table","text":"<p>Use this table to find the right pretrained model for your project and the correct preprocessing function.</p> Model Framework Import Preprocess Function Notes ResNet50 PyTorch <code>torchvision.models.resnet50()</code> <code>transforms.Normalize(mean, std)</code> Default 224\u00d7224 TensorFlow <code>tf.keras.applications.ResNet50</code> <code>preprocess_input</code> MobileNetV2 PyTorch <code>torchvision.models.mobilenet_v2()</code> <code>Normalize(mean, std)</code> Lightweight TensorFlow <code>tf.keras.applications.MobileNetV2</code> <code>preprocess_input</code> EfficientNetB0 PyTorch <code>torchvision.models.efficientnet_b0()</code> <code>Normalize(mean, std)</code> Good accuracy/speed TensorFlow <code>tf.keras.applications.EfficientNetB0</code> <code>preprocess_input</code> VGG16 PyTorch <code>torchvision.models.vgg16()</code> <code>Normalize(mean, std)</code> Large and old TensorFlow <code>tf.keras.applications.VGG16</code> <code>preprocess_input</code> <p>\ud83d\udd17 All models available at:</p> <ul> <li>PyTorch Vision Models</li> <li>TensorFlow Keras Applications</li> </ul>"},{"location":"appendices/#appendix-e-sample-projects-and-mini-exercises-per-chapter","title":"Appendix E. Sample Projects and Mini-Exercises per Chapter","text":"<p>Each chapter includes a practical checkpoint or challenge. Here\u2019s a recap to revisit later:</p> Chapter Exercise 1. Image Input Convert an image (JPG) to a 3D tensor and visualize its shape. 2. Tensors Practice reshaping, permuting, and broadcasting tensors. 3. Input Pipeline Write a full image \u2192 tensor \u2192 model input pipeline. 4. Preprocessing Visualize how normalization and resize affect your image. 5. Pretrained Models Try ResNet50 on a custom image and print predictions. 6. Datasets Load a folder of images using <code>ImageFolder</code> or <code>image_dataset_from_directory</code>. 7. Augmentation Apply at least 3 different augmentations and view the effects. 8. CNN Layers Build a custom <code>Conv2d \u2192 ReLU \u2192 MaxPool</code> pipeline manually. 9. CNN Terms Draw out how kernel, stride, and padding affect feature map size. 10. Forward/Call Implement <code>forward()</code> in PyTorch or <code>call()</code> in TensorFlow. 11. Summary Inspect model parameters and freeze layers selectively. 12. Build CNN Build and train a LeNet-style CNN from scratch. 13. Training Write a full training loop or use <code>model.fit()</code>. 14. Fine-Tuning Load MobileNetV2, freeze base, and train a new classifier head. 15. Generalization Implement early stopping and visualize overfitting. 16. Eval Mode Compare predictions in <code>train()</code> vs <code>eval()</code> mode. 17. Feature Maps Visualize intermediate layer outputs with hooks or submodels. 18. Inference Pipeline Build a reusable inference pipeline that handles image \u2192 model prediction. 19. Debug Checklist Pick a broken model and fix it using the debugging checklist."},{"location":"appendices/#final-words","title":"Final Words","text":"<p>These appendices complete your toolkit. Whether you're writing training code from scratch, deploying models, or debugging strange edge cases, this section will bring you back on track.</p> <p>Next Steps?</p> <ul> <li>Bookmark this cheatsheet.</li> <li>Try the mini-exercises if you skipped any.</li> <li>Build and deploy a real project using one of the pretrained models.</li> </ul> <p>If you\u2019ve reached this far\u2014you\u2019re not just reading a book. You\u2019ve trained your own mind like a neural net: layer by layer, with careful activation.</p> <p>Let\u2019s keep building.</p>"},{"location":"chapter1/","title":"Chapter 1: How a Neural Network Sees an Image","text":"<p>\u201cBefore the model learns, it sees. Before it classifies, it computes. And what it sees\u2014starts with pixels, channels, and shapes.\u201d</p>"},{"location":"chapter1/#why-this-chapter-matters","title":"Why This Chapter Matters","text":"<p>Every computer vision journey begins with an image. But here\u2019s the twist: your neural network doesn\u2019t see an image the way you do. It sees numbers. And not just any numbers\u2014tensors of pixel values, reshaped and normalized to fit the model\u2019s expectations.</p> <p>If you\u2019ve ever run into errors like:</p> <ul> <li> <p>\u201cExpected 3 channels, got 1\u201d</p> </li> <li> <p>\u201cShape mismatch: [1, 224, 224, 3] vs [3, 224, 224]\u201d</p> </li> <li> <p>\u201cModel output is garbage despite clean code\u201d</p> </li> </ul> <p>\u2026then it probably started here: the image-to-tensor pipeline wasn\u2019t correctly handled.</p> <p>In this chapter, we\u2019ll unpack the complete transformation from a JPEG or PNG file on disk to a model-ready tensor in memory. We\u2019ll go step by step\u2014from pixel arrays \u2192 float tensors \u2192 properly shaped inputs\u2014and explain how frameworks like PyTorch and TensorFlow treat the process differently.</p> <p>You\u2019ll see what the model sees. And that understanding will anchor everything you build later.</p>"},{"location":"chapter1/#conceptual-breakdown","title":"Conceptual Breakdown","text":"<p>\ud83d\udd39 What Is an Image in Memory?</p> <p>To a neural network, an image is just a 3D array\u2014Height, Width, and Color Channels (usually RGB). For grayscale, it\u2019s just H\u00d7W. For RGB, it\u2019s H\u00d7W\u00d73.</p> <p>But raw image files (JPEG, PNG) are compressed formats. To use them in training, we:</p> <ol> <li> <p>Load the image into memory</p> </li> <li> <p>Convert it to an array of pixel values (0\u2013255)</p> </li> <li> <p>Normalize/scale those values (e.g., 0.0 to 1.0 or with mean/std)</p> </li> <li> <p>Reshape it into a tensor format the model expects</p> </li> </ol> <p>Each step matters. A mismatch in any of these can wreck your model.</p> <p>\ud83d\udd39 Tensor Layouts: [H, W, C] vs [C, H, W]</p> <p>Different frameworks use different conventions:</p> <ul> <li> <p>TensorFlow uses <code>[Height, Width, Channels]</code></p> </li> <li> <p>PyTorch uses <code>[Channels, Height, Width]</code></p> </li> </ul> <p>The reason? Internal memory layout optimizations. But for you, it means that converting between these shapes is crucial when preparing images for your models.</p> <p>\ud83d\udd39 Model Input Shape: Why It Matters</p> <p>Neural networks are strict about input shape:</p> <ul> <li> <p>ResNet, MobileNet, EfficientNet, etc. expect a specific input size and layout</p> </li> <li> <p>Channels must match: grayscale (1), RGB (3), etc.</p> </li> <li> <p>Batch dimension must exist: <code>[1, C, H, W]</code> or <code>[1, H, W, C]</code></p> </li> </ul> <p>Even for a single image, you must simulate a batch\u2014most models don\u2019t accept raw 3D tensors.</p> <p>\ud83d\udd39 Visual Walkthrough: Image \u2192 Tensor \u2192 Model</p> <p>Let\u2019s break down what happens: <pre><code>Image File (e.g., 'dog.png')\n      \u2193\nLoad into memory (PIL / tf.io / OpenCV)\n      \u2193\nConvert to NumPy or Tensor (shape: H\u00d7W\u00d73)\n      \u2193\nNormalize (e.g., /255.0 or mean/std)\n      \u2193\nTranspose (if using PyTorch: \u2192 C\u00d7H\u00d7W)\n      \u2193\nAdd batch dim (\u2192 1\u00d7C\u00d7H\u00d7W or 1\u00d7H\u00d7W\u00d7C)\n      \u2193\nFeed to CNN\n</code></pre></p>"},{"location":"chapter1/#pytorch-implementation","title":"PyTorch Implementation","text":"<p>Here\u2019s how you go from image file to model-ready tensor in PyTorch: <pre><code>from PIL import Image\nimport torchvision.transforms as T\n\n# 1. Load image\nimage = Image.open(\"dog.png\").convert(\"RGB\")\n\n# 2. Define transform pipeline\ntransform = T.Compose([\n    T.Resize((224, 224)),\n    T.ToTensor(),  # Converts to [0,1] and switches to [C, H, W]\n    T.Normalize(mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225])  # Pretrained model mean/std\n])\n\n# 3. Apply transforms\ntensor = transform(image)  # shape: [3, 224, 224]\n\n# 4. Add batch dimension\ninput_tensor = tensor.unsqueeze(0)  # shape: [1, 3, 224, 224]\n</code></pre></p>"},{"location":"chapter1/#tensorflow-implementation","title":"TensorFlow Implementation","text":"<p>The same pipeline in TensorFlow looks like this: <pre><code>import tensorflow as tf\n\n# 1. Load image\nimage = tf.io.read_file(\"dog.png\")\nimage = tf.image.decode_png(image, channels=3)\n\n# 2. Resize and convert to float32\nimage = tf.image.resize(image, [224, 224])\nimage = tf.cast(image, tf.float32) / 255.0\n\n# 3. Normalize\nmean = tf.constant([0.485, 0.456, 0.406])\nstd = tf.constant([0.229, 0.224, 0.225])\nimage = (image - mean) / std\n\n# 4. Add batch dimension\ninput_tensor = tf.expand_dims(image, axis=0)  # shape: [1, 224, 224, 3]\n</code></pre></p>"},{"location":"chapter1/#framework-comparison-table","title":"Framework Comparison Table","text":"Step PyTorch TensorFlow Load image <code>PIL.Image.open()</code> <code>tf.io.read_file() + tf.image.decode</code> Resize <code>T.Resize((H, W))</code> <code>tf.image.resize()</code> Convert to float <code>T.ToTensor()</code> (scales to 0\u20131) <code>tf.cast(..., tf.float32) / 255.0</code> Normalize <code>T.Normalize(mean, std)</code> Manual: <code>(image - mean) / std</code> Layout <code>[C, H, W]</code> <code>[H, W, C]</code> Add batch dim <code>.unsqueeze(0)</code> <code>tf.expand_dims(..., axis=0)</code>"},{"location":"chapter1/#mini-exercise","title":"Mini-Exercise","text":"<p>Choose any image file and:</p> <ol> <li> <p>Load and visualize the original</p> </li> <li> <p>Convert it to a tensor using both PyTorch and TensorFlow</p> </li> <li> <p>Apply normalization</p> </li> <li> <p>Print shape at each step</p> </li> <li> <p>Confirm final shape matches model input requirement</p> </li> </ol> <p>Bonus: Try visualizing the image after normalization. What do the pixel values look like now?</p>"},{"location":"chapter10/","title":"\ud83d\udcd8 Chapter 10: Writing the <code>forward()</code> / <code>call()</code> Function","text":"<p>\u201cThis is where it all flows. The forward pass isn\u2019t just where your model computes\u2014it\u2019s where it thinks.\u201d</p>"},{"location":"chapter10/#why-this-chapter-matters","title":"Why This Chapter Matters","text":"<p>Designing a CNN is one thing\u2014executing it is another. All the layers you define? They don\u2019t mean anything until you stitch them together in a forward flow of logic.</p> <p>This chapter teaches you how to:</p> <ul> <li>Build custom models using <code>nn.Module</code> and <code>tf.keras.Model</code></li> <li>Control data flow through convolutional, pooling, and linear layers</li> <li>Debug shape mismatches through layer-by-layer tracking</li> <li>Make your model clean, modular, and ready for real-world deployment</li> </ul> <p>When implemented right, your model becomes not just correct\u2014but elegant and debuggable.</p>"},{"location":"chapter10/#conceptual-breakdown","title":"Conceptual Breakdown","text":""},{"location":"chapter10/#what-is-the-forward-call-function","title":"\ud83d\udd39 What Is the <code>forward()</code> / <code>call()</code> Function?","text":"<p>This function defines how your model processes data. It tells the model:</p> <ul> <li>What layers to use</li> <li>In what order to apply them</li> <li>What transformations to perform</li> </ul> <p>\ud83d\udccc Think of this function as the \u201cneural circuit diagram.\u201d</p>"},{"location":"chapter10/#pytorch-forward","title":"\ud83d\udd39 PyTorch: <code>forward()</code>","text":"<ul> <li>Defined inside a subclass of <code>nn.Module</code></li> <li>Called automatically during training or inference</li> </ul> <pre><code>class MyModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Conv2d(3, 16, 3)\n        self.fc = nn.Linear(16 * 222 * 222, 10)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = F.relu(x)\n        x = x.view(x.size(0), -1)\n        return self.fc(x)\n</code></pre>"},{"location":"chapter10/#tensorflow-call","title":"\ud83d\udd39 TensorFlow: <code>call()</code>","text":"<ul> <li>Defined inside a subclass of <code>tf.keras.Model</code></li> <li>Called implicitly during training (<code>fit</code>) or explicitly with <code>model(x)</code></li> </ul> <pre><code>class MyModel(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.conv = tf.keras.layers.Conv2D(16, 3)\n        self.flatten = tf.keras.layers.Flatten()\n        self.fc = tf.keras.layers.Dense(10)\n\n    def call(self, x):\n        x = self.conv(x)\n        x = tf.nn.relu(x)\n        x = self.flatten(x)\n        return self.fc(x)\n</code></pre>"},{"location":"chapter10/#best-practices","title":"\ud83d\udd39 Best Practices","text":"Tip Why It Matters Keep your model class clean Separates definition from execution Use <code>nn.Sequential</code> or functional blocks Reuse logic and reduce clutter Print shapes in <code>forward()</code> Helps catch shape mismatches early Use <code>x.view(x.size(0), -1)</code> (PyTorch) or <code>Flatten()</code> (TF) before Dense Proper vector flattening for FC layers"},{"location":"chapter10/#pytorch-implementation","title":"PyTorch Implementation","text":"<p>Let\u2019s create a full CNN with modular, readable <code>forward()</code> logic.</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ConvClassifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        # Feature extractor\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n\n        # Classifier\n        self.classifier = nn.Sequential(\n            nn.Flatten(),  # OR: use .view() in forward\n            nn.Linear(64 * 56 * 56, 128),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(128, 10)\n        )\n\n    def forward(self, x):\n        print(\"Input:\", x.shape)\n        x = self.features(x)\n        print(\"After Conv Layers:\", x.shape)\n        x = self.classifier(x)\n        print(\"Output:\", x.shape)\n        return x\n</code></pre> <p>\ud83d\udccc This model assumes input shape <code>[B, 3, 224, 224]</code>. You can adjust the classifier input size if using a different input resolution.</p>"},{"location":"chapter10/#tensorflow-implementation","title":"TensorFlow Implementation","text":"<p>Now the same logic in TensorFlow using <code>tf.keras.Model</code> subclassing:</p> <pre><code>import tensorflow as tf\n\nclass ConvClassifier(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n\n        self.conv1 = tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu')\n        self.pool1 = tf.keras.layers.MaxPooling2D(2)\n\n        self.conv2 = tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu')\n        self.pool2 = tf.keras.layers.MaxPooling2D(2)\n\n        self.flatten = tf.keras.layers.Flatten()\n        self.dense1 = tf.keras.layers.Dense(128, activation='relu')\n        self.dropout = tf.keras.layers.Dropout(0.5)\n        self.dense2 = tf.keras.layers.Dense(10)\n\n    def call(self, x, training=False):\n        print(\"Input:\", x.shape)\n        x = self.conv1(x)\n        x = self.pool1(x)\n\n        x = self.conv2(x)\n        x = self.pool2(x)\n        print(\"After Conv Layers:\", x.shape)\n\n        x = self.flatten(x)\n        x = self.dense1(x)\n        x = self.dropout(x, training=training)\n        return self.dense2(x)\n</code></pre>"},{"location":"chapter10/#debugging-tip-shape-by-shape-logging","title":"Debugging Tip: Shape-by-Shape Logging","text":"<p>To debug model flow:</p> <ul> <li>In PyTorch, use <code>print(x.shape)</code> inside <code>forward()</code></li> <li>In TensorFlow, use <code>print(x.shape)</code> inside <code>call()</code></li> </ul>"},{"location":"chapter10/#framework-comparison-table","title":"Framework Comparison Table","text":"Task PyTorch TensorFlow Model base class <code>nn.Module</code> <code>tf.keras.Model</code> Define layers In <code>__init__()</code> In <code>__init__()</code> Execute layers In <code>forward(self, x)</code> In <code>call(self, x)</code> Activation example <code>F.relu(x)</code> or <code>nn.ReLU()</code> <code>tf.nn.relu(x)</code> or <code>activation='relu'</code> Flatten <code>x.view(x.size(0), -1)</code> or <code>nn.Flatten()</code> <code>Flatten()</code> layer Dropout (training only) <code>nn.Dropout(p)</code> (active only in <code>.train()</code>) <code>Dropout()(x, training=training)</code>"},{"location":"chapter10/#mini-exercise","title":"Mini-Exercise","text":"<p>Create a small CNN model using both frameworks that:</p> <ol> <li>Accepts input shape <code>[1, 3, 64, 64]</code> (PyTorch) or <code>[1, 64, 64, 3]</code> (TF)</li> <li> <p>Contains:</p> </li> <li> <p>Two <code>Conv2D</code> layers</p> </li> <li><code>ReLU</code> + <code>MaxPool</code> after each</li> <li>One <code>Flatten</code> + <code>Dense</code> + <code>Dropout</code></li> <li>Prints shape at each stage of <code>forward()</code> or <code>call()</code></li> <li>Returns logits for 5 classes</li> </ol> <p>Bonus: Add a conditional block to test <code>training=True</code> for dropout behavior in TensorFlow.</p>"},{"location":"chapter11/","title":"Chapter 11: Model Summary and Parameter Inspection","text":"<p>\u201cUnderstanding your model is like reading blueprints before construction. Don\u2019t train it blind\u2014inspect, analyze, and optimize.\u201d</p>"},{"location":"chapter11/#why-this-chapter-matters","title":"Why This Chapter Matters","text":"<p>You\u2019ve built a CNN and defined its forward flow. But now you need to:</p> <ul> <li>Check if the architecture matches your expectations</li> <li>Inspect layer shapes and total parameter counts</li> <li>Freeze or unfreeze layers for transfer learning</li> <li>Load/save weights or extract specific layer outputs</li> </ul> <p>This chapter helps you understand:</p> <ul> <li>How to summarize your model</li> <li>How to access weights and parameters</li> <li>How to manage layers for training or inference</li> </ul> <p>You\u2019ll learn to treat models not as magic boxes\u2014but as transparent, inspectable systems.</p>"},{"location":"chapter11/#conceptual-breakdown","title":"Conceptual Breakdown","text":""},{"location":"chapter11/#what-to-inspect-in-a-model","title":"\ud83d\udd39 What to Inspect in a Model","text":"Property Why it Matters Layer names/types Ensure architecture is correct Output shapes Catch shape mismatches early Total parameters Know model size and overfitting risk Trainable vs frozen Required for transfer learning/fine-tuning Weight values For debugging, initialization checks"},{"location":"chapter11/#freezing-vs-unfreezing-layers","title":"\ud83d\udd39 Freezing vs Unfreezing Layers","text":"<p>Freezing a layer means its weights won\u2019t update during training (used in transfer learning). Unfreezing means allowing gradient flow again.</p> <p>\ud83d\udccc Freeze base layers \u2192 train top layers only \u2192 unfreeze gradually.</p>"},{"location":"chapter11/#pytorch-implementation","title":"PyTorch Implementation","text":""},{"location":"chapter11/#model-summary","title":"\ud83d\udd38 Model Summary","text":"<p>Use the <code>torchsummary</code> package (or print manually):</p> <pre><code>pip install torchsummary\n</code></pre> <pre><code>from torchsummary import summary\nimport torch\n\nmodel = ConvClassifier()\nsummary(model, input_size=(3, 224, 224))\n</code></pre>"},{"location":"chapter11/#inspect-parameters","title":"\ud83d\udd38 Inspect Parameters","text":"<pre><code>for name, param in model.named_parameters():\n    print(f\"{name}: {param.shape}, requires_grad={param.requires_grad}\")\n</code></pre>"},{"location":"chapter11/#freeze-layers","title":"\ud83d\udd38 Freeze Layers","text":"<pre><code>for param in model.features.parameters():\n    param.requires_grad = False\n</code></pre> <p>To unfreeze later:</p> <pre><code>for param in model.features.parameters():\n    param.requires_grad = True\n</code></pre>"},{"location":"chapter11/#saveload-weights","title":"\ud83d\udd38 Save/Load Weights","text":"<pre><code>torch.save(model.state_dict(), \"model_weights.pth\")\n\nmodel.load_state_dict(torch.load(\"model_weights.pth\"))\nmodel.eval()  # Set to inference mode\n</code></pre>"},{"location":"chapter11/#tensorflow-implementation","title":"TensorFlow Implementation","text":""},{"location":"chapter11/#model-summary_1","title":"\ud83d\udd38 Model Summary","text":"<pre><code>model = ConvClassifier()\nmodel.build(input_shape=(None, 224, 224, 3))\nmodel.summary()\n</code></pre>"},{"location":"chapter11/#inspect-weights","title":"\ud83d\udd38 Inspect Weights","text":"<pre><code>for layer in model.layers:\n    print(layer.name, layer.trainable)\n    for weight in layer.weights:\n        print(f\"  {weight.name} - shape: {weight.shape}\")\n</code></pre>"},{"location":"chapter11/#freeze-layers_1","title":"\ud83d\udd38 Freeze Layers","text":"<pre><code>for layer in model.layers:\n    layer.trainable = False  # freeze\n</code></pre> <p>Unfreeze:</p> <pre><code>for layer in model.layers:\n    layer.trainable = True\n</code></pre>"},{"location":"chapter11/#saveload-weights_1","title":"\ud83d\udd38 Save/Load Weights","text":"<pre><code>model.save_weights(\"model_checkpoint.h5\")\n\n# Reload weights into the same architecture\nmodel.load_weights(\"model_checkpoint.h5\")\n</code></pre>"},{"location":"chapter11/#pytorch-vs-tensorflow-parameter-access","title":"PyTorch vs TensorFlow Parameter Access","text":"Task PyTorch TensorFlow Get all weights <code>model.parameters()</code> <code>model.weights</code> Get named weights <code>model.named_parameters()</code> <code>layer.weights</code> per layer Freeze training <code>param.requires_grad = False</code> <code>layer.trainable = False</code> Save weights <code>torch.save(state_dict)</code> <code>model.save_weights()</code> Load weights <code>model.load_state_dict(...)</code> <code>model.load_weights()</code>"},{"location":"chapter11/#model-modes-train-vs-eval","title":"Model Modes: Train vs Eval","text":"Mode PyTorch TensorFlow Training <code>model.train()</code> <code>training=True</code> in <code>call()</code> Inference <code>model.eval()</code> <code>training=False</code> in <code>call()</code> Dropout/BNorm Behave differently in modes Same applies in both frameworks <p>Always remember:</p> <ul> <li>Use <code>model.eval()</code> in PyTorch during inference (turns off dropout, uses running stats in BatchNorm).</li> <li>In TensorFlow, use <code>training=False</code> explicitly in <code>call()</code>.</li> </ul>"},{"location":"chapter11/#mini-exercise","title":"Mini-Exercise","text":"<ol> <li>Build a small CNN for 3-class classification.</li> <li>Print full model summary.</li> <li>Freeze all convolutional layers.</li> <li>Confirm only <code>Linear</code> / <code>Dense</code> layers are trainable.</li> <li>Save and reload weights.</li> <li>Switch between train/eval modes and print dropout effect.</li> </ol> <p>Bonus: Write a utility function that counts:</p> <ul> <li>Total parameters</li> <li>Trainable parameters</li> <li>Frozen parameters</li> </ul>"},{"location":"chapter12/","title":"Chapter 12: Building Your First CNN: Patterns and Pitfalls","text":"<p>\u201cGood CNNs don\u2019t come from just stacking layers\u2014they come from knowing why you stack them.\u201d</p>"},{"location":"chapter12/#why-this-chapter-matters","title":"Why This Chapter Matters","text":"<p>You now understand:</p> <ul> <li>How images become tensors,</li> <li>How layers like Conv2D, Pooling, and BatchNorm work,</li> <li>How to write clean <code>forward()</code> or <code>call()</code> functions,</li> <li>How to inspect models and control parameters.</li> </ul> <p>This chapter helps you:</p> <ul> <li>Design your own architecture from scratch</li> <li>Follow proven CNN patterns</li> <li>Avoid common architectural mistakes</li> <li>Set yourself up for scaling later to deeper or pretrained models</li> </ul> <p>It's your first step toward becoming a deep learning architect\u2014not just a user.</p>"},{"location":"chapter12/#conceptual-breakdown","title":"Conceptual Breakdown","text":""},{"location":"chapter12/#what-makes-a-good-cnn","title":"\ud83d\udd39 What Makes a \"Good\" CNN?","text":"Good CNN Design Has\u2026 Why It Matters Clear separation of feature + classifier Easier to extend or replace sections Progressive increase in filter count Helps extract richer features deeper in network Downsampling at reasonable intervals Balances spatial resolution vs computation Non-linearities + normalization Improves gradient flow, training stability Proper flattening before dense layers Ensures correct classifier input shape"},{"location":"chapter12/#classic-cnn-design-patterns","title":"\ud83d\udd39 Classic CNN Design Patterns","text":""},{"location":"chapter12/#lenet-1998","title":"\ud83e\uddf1 LeNet (1998)","text":"<ul> <li>Small filters (5\u00d75), low depth</li> <li>MaxPooling for downsampling</li> <li>Fully connected at the end</li> </ul> <pre><code>INPUT \u2192 Conv \u2192 ReLU \u2192 Pool \u2192 Conv \u2192 ReLU \u2192 Pool \u2192 FC \u2192 FC \u2192 Softmax\n</code></pre>"},{"location":"chapter12/#mini-vgg-style","title":"\ud83e\uddf1 Mini-VGG Style","text":"<ul> <li>Use stacks of 3\u00d73 Conv layers before pooling</li> <li>Double filters after each pooling</li> <li>No FC until final layers</li> </ul> <pre><code>INPUT \u2192 [Conv \u2192 ReLU] x2 \u2192 Pool \u2192 [Conv \u2192 ReLU] x2 \u2192 Pool \u2192 FC \u2192 Softmax\n</code></pre> <p>\ud83d\udccc Rule of thumb: Double filters, halve resolution after each pool</p>"},{"location":"chapter12/#choosing-filter-sizes","title":"\ud83d\udd39 Choosing Filter Sizes","text":"Kernel Size Best For 1\u00d71 Reducing/increasing channel depth 3\u00d73 Most common, efficient pattern capture 5\u00d75 Broader patterns, but costlier <p>\ud83d\udccc Stack 2\u00d7 3\u00d73 layers instead of one 5\u00d75 (same receptive field, fewer params)</p>"},{"location":"chapter12/#when-to-use-pooling","title":"\ud83d\udd39 When to Use Pooling","text":"<ul> <li>Use MaxPooling2D or stride=2 Conv2D to downsample</li> <li>Common after 1 or 2 Conv blocks</li> <li>Helps reduce computation and adds invariance to translation</li> </ul> <p>\ud83d\udccc Avoid pooling too early\u2014keep spatial detail in early layers</p>"},{"location":"chapter12/#flattening-correctly","title":"\ud83d\udd39 Flattening Correctly","text":"<ul> <li>PyTorch: <code>.view(x.size(0), -1)</code> or <code>nn.Flatten()</code></li> <li>TensorFlow: <code>Flatten()</code> layer</li> </ul> <p>You can also use <code>AdaptiveAvgPool2d((1, 1))</code> or <code>GlobalAveragePooling2D()</code> to remove dependence on input image size.</p>"},{"location":"chapter12/#common-mistakes-to-avoid","title":"\ud83d\udd39 Common Mistakes to Avoid","text":"Mistake Consequence Forgetting <code>.view()</code> / <code>.Flatten()</code> Shape error in Linear/Dense layer Pooling too early or too often Loss of spatial detail, underfitting Too few filters Not enough capacity to learn visual patterns Mismatched shapes at classifier input Crash at final FC layer No normalization or activation Poor learning and convergence"},{"location":"chapter12/#pytorch-build-a-clean-minicnn","title":"\ud83d\udcbb PyTorch: Build a Clean MiniCNN","text":"<pre><code>import torch\nimport torch.nn as nn\n\nclass MiniCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64 * 56 * 56, 128),  # assuming input is 224\u00d7224\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 3)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        return self.classifier(x)\n</code></pre>"},{"location":"chapter12/#tensorflow-equivalent-minicnn","title":"\ud83e\uddea TensorFlow: Equivalent MiniCNN","text":"<pre><code>import tensorflow as tf\nfrom tensorflow.keras import layers, models\n\nclass MiniCNN(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n\n        self.conv1 = layers.Conv2D(32, 3, padding='same', activation='relu')\n        self.pool1 = layers.MaxPooling2D()\n\n        self.conv2 = layers.Conv2D(64, 3, padding='same', activation='relu')\n        self.pool2 = layers.MaxPooling2D()\n\n        self.flatten = layers.Flatten()\n        self.fc1 = layers.Dense(128, activation='relu')\n        self.dropout = layers.Dropout(0.3)\n        self.out = layers.Dense(3)\n\n    def call(self, x, training=False):\n        x = self.conv1(x)\n        x = self.pool1(x)\n\n        x = self.conv2(x)\n        x = self.pool2(x)\n\n        x = self.flatten(x)\n        x = self.fc1(x)\n        x = self.dropout(x, training=training)\n        return self.out(x)\n</code></pre>"},{"location":"chapter12/#framework-comparison-table","title":"Framework Comparison Table","text":"Element PyTorch TensorFlow Conv + ReLU + Pool block <code>nn.Sequential()</code> + <code>nn.Conv2d</code>, etc. <code>layers.Conv2D</code> + <code>ReLU</code> + <code>MaxPooling</code> Flatten + Dense <code>nn.Flatten()</code> + <code>nn.Linear</code> <code>Flatten()</code> + <code>Dense()</code> Dropout in training Auto-disabled in <code>eval()</code> mode Manual: <code>training=True</code> in <code>call()</code> Global pooling <code>AdaptiveAvgPool2d((1, 1))</code> <code>GlobalAveragePooling2D()</code>"},{"location":"chapter12/#mini-exercise","title":"Mini-Exercise","text":"<p>Design a CNN for CIFAR-10 (input: 32\u00d732\u00d73):</p> <ol> <li>Stack 3 Conv2D layers with increasing filters (e.g., 32 \u2192 64 \u2192 128)</li> <li>Add ReLU + MaxPool after every 2 layers</li> <li>Use Global Average Pooling before Dense</li> <li>Use Dropout to prevent overfitting</li> <li>Output 10 classes</li> </ol> <p>Bonus: Replace MaxPool2D with <code>stride=2</code> Conv2D and compare performance.</p>"},{"location":"chapter13/","title":"Chapter 13: Loss Functions and Optimizers","text":"<p>\u201cA network doesn\u2019t improve by magic\u2014it learns by failing. The loss is the pain, the optimizer is the cure.\u201d</p>"},{"location":"chapter13/#why-this-chapter-matters","title":"Why This Chapter Matters","text":"<p>When training a CNN, we want it to:</p> <ul> <li>Make better predictions over time</li> <li>Improve by adjusting its weights through gradients</li> </ul> <p>But how do we quantify wrong predictions? That\u2019s where loss functions come in.</p> <p>And once we have loss, how do we adjust the network to reduce it? That\u2019s the job of optimizers.</p> <p>Together, they are the learning engine of your CNN:</p> <ul> <li>The loss function tells the model how wrong it is.</li> <li>The optimizer updates the weights to make it less wrong next time.</li> </ul> <p>Understanding both is vital for:</p> <ul> <li>Choosing the right learning strategy</li> <li>Debugging model collapse or instability</li> <li>Fine-tuning pretrained models</li> </ul>"},{"location":"chapter13/#conceptual-breakdown","title":"Conceptual Breakdown","text":""},{"location":"chapter13/#what-is-a-loss-function","title":"\ud83d\udd39 What Is a Loss Function?","text":"<p>A loss function computes the difference between:</p> <ul> <li>The model\u2019s predicted output (logits/probabilities)</li> <li>The true label (target)</li> </ul> <p>The output is a scalar, which is differentiable so gradients can flow.</p>"},{"location":"chapter13/#types-of-loss-functions","title":"\ud83d\udd39 Types of Loss Functions","text":"Loss Function Use Case PyTorch TensorFlow CrossEntropyLoss Multi-class classification <code>nn.CrossEntropyLoss()</code> <code>SparseCategoricalCrossentropy()</code> BCEWithLogitsLoss Binary classification (with logits) <code>nn.BCEWithLogitsLoss()</code> <code>BinaryCrossentropy(from_logits=True)</code> MSELoss Regression or feature matching <code>nn.MSELoss()</code> <code>MeanSquaredError()</code> <p>\ud83d\udccc For classification tasks:</p> <ul> <li>Use CrossEntropyLoss if your model outputs raw logits</li> <li>For softmax outputs, use CategoricalCrossentropy without logits</li> </ul>"},{"location":"chapter13/#what-is-an-optimizer","title":"\ud83d\udd39 What Is an Optimizer?","text":"<p>An optimizer updates weights using the gradients computed via backpropagation.</p> <p>Optimizers apply:</p> <ul> <li>Learning rate (<code>lr</code>)</li> <li>Momentum, adaptive steps, or regularization</li> </ul>"},{"location":"chapter13/#common-optimizers","title":"\ud83d\udd39 Common Optimizers","text":"Optimizer Behavior Best For SGD Basic gradient descent Simple, interpretable tasks SGD + Momentum Adds velocity to updates Faster convergence Adam Adaptive step size per parameter Most deep learning tasks RMSprop Like Adam but simpler Good for noisy gradients (e.g., RNNs) <p>\ud83d\udccc Start with Adam. Move to SGD + Momentum for fine-tuning large models.</p>"},{"location":"chapter13/#visualizing-gradient-flow","title":"\ud83d\udd39 Visualizing Gradient Flow","text":"<p>Think of:</p> <ul> <li>Loss as elevation</li> <li>Gradients as slope</li> <li>Optimizer as the hiker moving downhill</li> </ul> <p>Bad loss or bad optimizer = stuck in a valley Good setup = smooth descent to a better model</p>"},{"location":"chapter13/#pytorch-implementation","title":"PyTorch Implementation","text":""},{"location":"chapter13/#crossentropy-loss-adam-optimizer","title":"\ud83d\udd38 CrossEntropy Loss + Adam Optimizer","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nmodel = MiniCNN()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Dummy training step\nfor images, labels in train_loader:\n    optimizer.zero_grad()        # Reset gradients\n    outputs = model(images)      # Forward pass\n    loss = criterion(outputs, labels)  # Compute loss\n    loss.backward()              # Backpropagation\n    optimizer.step()             # Update weights\n</code></pre>"},{"location":"chapter13/#switch-to-sgd-with-momentum","title":"\ud83d\udd38 Switch to SGD with Momentum","text":"<pre><code>optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n</code></pre>"},{"location":"chapter13/#tensorflow-implementation","title":"TensorFlow Implementation","text":""},{"location":"chapter13/#crossentropy-loss-adam-optimizer_1","title":"\ud83d\udd38 CrossEntropy Loss + Adam Optimizer","text":"<pre><code>import tensorflow as tf\n\nmodel = MiniCNN()\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n\n# Dummy training step (eager)\nwith tf.GradientTape() as tape:\n    predictions = model(images, training=True)\n    loss = loss_fn(labels, predictions)\ngrads = tape.gradient(loss, model.trainable_variables)\noptimizer.apply_gradients(zip(grads, model.trainable_variables))\n</code></pre>"},{"location":"chapter13/#use-sgd-instead","title":"\ud83d\udd38 Use SGD Instead","text":"<pre><code>optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n</code></pre>"},{"location":"chapter13/#framework-comparison-table","title":"Framework Comparison Table","text":"Component PyTorch TensorFlow Loss Function <code>nn.CrossEntropyLoss()</code> <code>tf.keras.losses.SparseCategoricalCrossentropy()</code> Loss with logits Built-in (raw outputs supported) <code>from_logits=True</code> Optimizer <code>optim.Adam(...)</code> <code>tf.keras.optimizers.Adam(...)</code> Update weights <code>loss.backward()</code> + <code>optimizer.step()</code> <code>GradientTape()</code> + <code>apply_gradients()</code> Zero gradients <code>optimizer.zero_grad()</code> Automatic inside tape"},{"location":"chapter13/#mini-exercise","title":"Mini-Exercise","text":"<ol> <li>Build a CNN for 10-class image classification</li> <li>Try both CrossEntropy + Adam and SGD + momentum</li> <li>Log loss per batch and plot curve after 5 epochs</li> <li> <p>Try tweaking:</p> </li> <li> <p>Learning rate (<code>lr</code>)</p> </li> <li>Loss function (<code>BCEWithLogitsLoss</code> for binary)</li> <li>Observe: Does one optimizer converge faster? Does one oscillate?</li> </ol> <p>Bonus: Add weight decay (L2 regularization) to SGD and test performance.</p>"},{"location":"chapter14/","title":"Chapter 14: Training Loop Mechanics","text":"<p>\u201cThis is where the magic happens\u2014not in the layers, not in the loss\u2014but in the loop where learning actually unfolds.\u201d</p>"},{"location":"chapter14/#why-this-chapter-matters","title":"Why This Chapter Matters","text":"<p>A CNN\u2019s training process is a loop\u2014a cycle that feeds data into the model, computes the loss, updates weights, and repeats across epochs. But training isn\u2019t just calling <code>.fit()</code> or <code>.train()</code> and walking away.</p> <p>You need to:</p> <ul> <li>Log losses and accuracy properly</li> <li>Save and restore checkpoints</li> <li>Debug silently failing models</li> <li>Use early stopping, learning rate schedules, and more</li> </ul> <p>This chapter gives you the tools to:</p> <ul> <li>Write custom, reproducible training loops</li> <li>Understand what happens at every step</li> <li>Monitor model progress and troubleshoot problems early</li> </ul>"},{"location":"chapter14/#conceptual-breakdown","title":"Conceptual Breakdown","text":""},{"location":"chapter14/#anatomy-of-a-training-loop","title":"\ud83d\udd39 Anatomy of a Training Loop","text":"<p>A complete training loop typically includes:</p> <ol> <li>Model in train mode</li> <li>Loop over epochs</li> <li>Loop over batches</li> <li>Forward pass through model</li> <li>Compute loss</li> <li>Backward pass (PyTorch) or gradient tape (TF)</li> <li>Update weights</li> <li>Track and log metrics</li> <li>Validate model at each epoch</li> </ol>"},{"location":"chapter14/#epoch-vs-batch","title":"\ud83d\udd39 Epoch vs Batch","text":"<ul> <li>Batch: A group of training examples processed together</li> <li>Epoch: One full pass over the entire training dataset</li> </ul> <p>\ud83d\udccc Loss typically fluctuates per batch but should trend downward across epochs.</p>"},{"location":"chapter14/#train-vs-validation","title":"\ud83d\udd39 Train vs Validation","text":"Phase Purpose Dropout/BNorm Active? Training Learn via gradient descent \u2705 Yes Validation Monitor generalization \u274c No"},{"location":"chapter14/#pytorch-full-training-loop","title":"PyTorch Full Training Loop","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nmodel = MiniCNN()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Set to training mode\nmodel.train()\n\nfor epoch in range(10):  # num_epochs\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for images, labels in train_loader:\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = outputs.max(1)\n        correct += predicted.eq(labels).sum().item()\n        total += labels.size(0)\n\n    acc = 100. * correct / total\n    print(f\"Epoch {epoch+1}, Loss: {running_loss:.3f}, Accuracy: {acc:.2f}%\")\n</code></pre>"},{"location":"chapter14/#add-validation","title":"\ud83d\udd38 Add Validation","text":"<pre><code>model.eval()  # turn off Dropout &amp; BatchNorm\nwith torch.no_grad():\n    for images, labels in val_loader:\n        outputs = model(images)\n        val_loss = criterion(outputs, labels)\n</code></pre>"},{"location":"chapter14/#tensorflow-full-training-loop","title":"TensorFlow Full Training Loop","text":"<pre><code>import tensorflow as tf\n\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\noptimizer = tf.keras.optimizers.Adam()\n\ntrain_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\nval_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n\nfor epoch in range(10):\n    print(f\"\\nEpoch {epoch + 1}\")\n\n    # TRAINING\n    for images, labels in train_ds:\n        with tf.GradientTape() as tape:\n            predictions = model(images, training=True)\n            loss = loss_fn(labels, predictions)\n        grads = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n        train_acc_metric.update_state(labels, predictions)\n\n    train_acc = train_acc_metric.result()\n    print(f\"Training accuracy: {train_acc:.4f}\")\n    train_acc_metric.reset_state()\n\n    # VALIDATION\n    for val_images, val_labels in val_ds:\n        val_preds = model(val_images, training=False)\n        val_acc_metric.update_state(val_labels, val_preds)\n\n    val_acc = val_acc_metric.result()\n    print(f\"Validation accuracy: {val_acc:.4f}\")\n    val_acc_metric.reset_state()\n</code></pre>"},{"location":"chapter14/#saving-checkpoints","title":"\ud83d\udd39 Saving Checkpoints","text":""},{"location":"chapter14/#pytorch","title":"PyTorch","text":"<pre><code>torch.save(model.state_dict(), \"checkpoint.pth\")\nmodel.load_state_dict(torch.load(\"checkpoint.pth\"))\n</code></pre>"},{"location":"chapter14/#tensorflow","title":"TensorFlow","text":"<pre><code>model.save_weights(\"checkpoint.h5\")\nmodel.load_weights(\"checkpoint.h5\")\n</code></pre>"},{"location":"chapter14/#early-stopping-and-learning-rate-scheduling","title":"\ud83d\udd39 Early Stopping and Learning Rate Scheduling","text":""},{"location":"chapter14/#pytorch_1","title":"PyTorch","text":"<pre><code>scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2)\n\n# After each epoch:\nscheduler.step(val_loss)\n</code></pre>"},{"location":"chapter14/#tensorflow_1","title":"TensorFlow","text":"<pre><code>callback_list = [\n    tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n    tf.keras.callbacks.ReduceLROnPlateau(patience=2)\n]\n\nmodel.fit(train_ds, validation_data=val_ds, callbacks=callback_list, epochs=10)\n</code></pre>"},{"location":"chapter14/#framework-comparison-table","title":"Framework Comparison Table","text":"Feature PyTorch TensorFlow Forward pass <code>outputs = model(images)</code> <code>preds = model(images, training=True)</code> Loss computation <code>loss = criterion(outputs, labels)</code> <code>loss_fn(labels, preds)</code> Backpropagation <code>loss.backward()</code> <code>tape.gradient(...)</code> Weight update <code>optimizer.step()</code> <code>apply_gradients(...)</code> Epoch logging Manual Metrics + custom logging Early stopping Manual or <code>torch_lr_finder</code> Built-in <code>callbacks</code>"},{"location":"chapter14/#mini-exercise","title":"Mini-Exercise","text":"<p>Create a complete training loop for a 3-class classification task:</p> <ol> <li>Use PyTorch or TensorFlow</li> <li> <p>Track:</p> </li> <li> <p>Training loss</p> </li> <li>Training accuracy</li> <li>Validation accuracy</li> <li> <p>Add:</p> </li> <li> <p>Early stopping</p> </li> <li>Reduce LR on plateau</li> <li>Model checkpoint saving</li> </ol> <p>Bonus: Plot training/validation accuracy per epoch using <code>matplotlib</code>.</p>"},{"location":"chapter15/","title":"Chapter 15: Training Strategies and Fine-Tuning Pretrained CNNs","text":"<p>\u201cA good model trains well. A great model generalizes. The difference is in your training strategy.\u201d</p>"},{"location":"chapter15/#why-this-chapter-matters","title":"Why This Chapter Matters","text":"<p>Even the best CNN architectures can fail if:</p> <ul> <li>You train too long</li> <li>You train the wrong layers</li> <li>You don't handle data imbalance</li> <li>You mismatch inputs with pretrained expectations</li> </ul> <p>This chapter will teach you how to:</p> <ul> <li>Freeze, fine-tune, and retrain CNNs correctly</li> <li>Apply regularization and learning rate schedules</li> <li>Handle imbalanced datasets the right way</li> <li>Recognize and respond to overfitting vs underfitting</li> </ul> <p>Whether you're training from scratch or adapting ResNet to classify medical images, this chapter gives you battle-tested practices for generalization-focused training.</p>"},{"location":"chapter15/#1-when-to-fine-tune-vs-freeze","title":"\ud83d\udd39 1. When to Fine-Tune vs Freeze","text":""},{"location":"chapter15/#base-layers-vs-top-layers","title":"\ud83d\udd38 Base Layers vs Top Layers","text":"<ul> <li>Base layers: Earlier convolutional blocks that detect general patterns (edges, corners, textures)</li> <li>Top layers: Deeper blocks and classifiers that detect task-specific patterns</li> </ul>"},{"location":"chapter15/#three-training-scenarios","title":"\ud83d\udd38 Three Training Scenarios","text":"Strategy What It Does When to Use Feature Extraction Freeze all convolutional layers, train classifier only Small custom dataset, fast prototyping Fine-Tuning (Top) Freeze early layers, train top conv + classifier Medium dataset, similar domain to ImageNet Full Retraining Train all layers Large dataset, significantly different domain"},{"location":"chapter15/#pytorch-implementation-freezing-layers","title":"\ud83d\udd38 PyTorch Implementation: Freezing Layers","text":"<pre><code># Freeze all layers\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Unfreeze top layers\nfor param in model.classifier.parameters():\n    param.requires_grad = True\n\nmodel.eval()  # important for correct BatchNorm and Dropout behavior\n</code></pre>"},{"location":"chapter15/#tensorflow-implementation-freezing-layers","title":"\ud83d\udd38 TensorFlow Implementation: Freezing Layers","text":"<pre><code># Freeze base model\nbase_model = tf.keras.applications.ResNet50(include_top=False, input_shape=(224, 224, 3))\nbase_model.trainable = False\n\n# Add top layers\nmodel = tf.keras.Sequential([\n    base_model,\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dense(3)  # your class count\n])\n</code></pre> <p>To unfreeze selectively:</p> <pre><code>for layer in base_model.layers[-20:]:  # Unfreeze last 20 layers\n    layer.trainable = True\n</code></pre>"},{"location":"chapter15/#2-adapting-pretrained-models","title":"\ud83d\udd39 2. Adapting Pretrained Models","text":""},{"location":"chapter15/#replace-output-layer","title":"\ud83d\udd38 Replace Output Layer","text":"<p>Most pretrained models end with Dense layers for 1000 ImageNet classes. You\u2019ll need to:</p> <ul> <li>Replace the last Dense/Linear layer</li> <li>Match your dataset\u2019s class count</li> </ul>"},{"location":"chapter15/#pytorch","title":"PyTorch","text":"<pre><code>model.fc = nn.Linear(model.fc.in_features, num_classes)\n</code></pre>"},{"location":"chapter15/#tensorflow","title":"TensorFlow","text":"<pre><code>model = tf.keras.Sequential([\n    base_model,\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dense(num_classes)\n])\n</code></pre>"},{"location":"chapter15/#use-adaptive-pooling-for-any-input-size","title":"\ud83d\udd38 Use Adaptive Pooling for Any Input Size","text":"<p>CNNs expect fixed-size inputs (e.g., 224\u00d7224), but you can:</p> <ul> <li>Use <code>AdaptiveAvgPool2d((1, 1))</code> in PyTorch</li> <li>Use <code>GlobalAveragePooling2D()</code> in TensorFlow</li> </ul> <p>These remove the dependence on fixed spatial dimensions.</p>"},{"location":"chapter15/#normalize-inputs-to-match-model-expectation","title":"\ud83d\udd38 Normalize Inputs to Match Model Expectation","text":"<p>If you use a pretrained ResNet or MobileNet:</p> <ul> <li>Match the mean/std normalization</li> <li>Use the correct channel order and value range</li> </ul> <p>See Chapter 5 for full details.</p>"},{"location":"chapter15/#3-regularization-techniques","title":"\ud83d\udd39 3. Regularization Techniques","text":"<p>Regularization helps prevent overfitting.</p>"},{"location":"chapter15/#dropout","title":"\ud83d\udd38 Dropout","text":"<ul> <li>Randomly drops neurons during training</li> <li>Use after Dense layers, not Convs</li> <li>Typical value: <code>0.3</code> to <code>0.5</code></li> </ul> <pre><code>nn.Dropout(0.5)  # PyTorch\nlayers.Dropout(0.5)  # TensorFlow\n</code></pre>"},{"location":"chapter15/#weight-decay-l2-regularization","title":"\ud83d\udd38 Weight Decay (L2 Regularization)","text":"<p>Applies penalty to large weights</p> Framework How to Use PyTorch <code>optim.Adam(..., weight_decay=1e-4)</code> TensorFlow Add kernel regularizer: <code>Dense(..., kernel_regularizer=l2(0.001))</code>"},{"location":"chapter15/#handling-data-imbalance","title":"\ud83d\udd38 Handling Data Imbalance","text":""},{"location":"chapter15/#1-class-weights-in-loss","title":"1. Class Weights in Loss","text":"<ul> <li>Assign higher weight to underrepresented classes</li> </ul> <pre><code># PyTorch\nweights = torch.tensor([1.0, 2.0, 0.5])  # adjust per class\ncriterion = nn.CrossEntropyLoss(weight=weights)\n</code></pre> <pre><code># TensorFlow\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\nmodel.compile(..., loss=loss_fn, class_weight={0:1.0, 1:2.0, 2:0.5})\n</code></pre>"},{"location":"chapter15/#2-oversampling","title":"2. Oversampling","text":"<ul> <li>Duplicate rare samples</li> <li>Can be done manually or via <code>WeightedRandomSampler</code> in PyTorch</li> </ul>"},{"location":"chapter15/#3-dataset-inspection","title":"3. Dataset Inspection","text":"<p>Always:</p> <ul> <li>Visualize sample counts per class</li> <li>Log confusion matrix during validation</li> </ul>"},{"location":"chapter15/#4-training-strategies-for-generalization","title":"\ud83d\udd39 4. Training Strategies for Generalization","text":""},{"location":"chapter15/#early-stopping","title":"\ud83d\udd38 Early Stopping","text":"<p>Stop training when validation stops improving</p> PyTorch TensorFlow Manual via patience counter <code>EarlyStopping(patience=3, restore_best_weights=True)</code>"},{"location":"chapter15/#learning-rate-schedules","title":"\ud83d\udd38 Learning Rate Schedules","text":"Strategy Purpose PyTorch TensorFlow StepLR Decays LR every N epochs <code>StepLR(optimizer, step_size=5, gamma=0.1)</code> <code>LearningRateScheduler</code> callback ReduceLROnPlateau Reduce LR when val loss plateaus <code>ReduceLROnPlateau(...)</code> <code>ReduceLROnPlateau(...)</code> Cosine Annealing Oscillates learning rate smoothly <code>CosineAnnealingLR(...)</code> Custom <code>LearningRateScheduler</code>"},{"location":"chapter15/#gradual-unfreezing","title":"\ud83d\udd38 Gradual Unfreezing","text":"<p>In fine-tuning:</p> <ul> <li>Start with base frozen</li> <li>Unfreeze one block at a time</li> <li>Reduce LR when unfreezing</li> </ul>"},{"location":"chapter15/#5-recognizing-overfitting-and-underfitting","title":"\ud83d\udd39 5. Recognizing Overfitting and Underfitting","text":""},{"location":"chapter15/#visual-clues-from-lossaccuracy-curves","title":"\ud83d\udd38 Visual Clues from Loss/Accuracy Curves","text":"Symptom Diagnosis Fix Suggestion High train acc, low val acc Overfitting Add regularization, more data, dropout Flat train + val accuracy Underfitting Increase model capacity or training time Val loss spikes upward Training too long Use early stopping <p>\ud83d\udccc Use <code>matplotlib</code> to plot:</p> <ul> <li><code>train_loss</code>, <code>val_loss</code>, <code>train_acc</code>, <code>val_acc</code> vs epoch</li> </ul>"},{"location":"chapter15/#framework-comparison-table","title":"Framework Comparison Table","text":"Concept PyTorch TensorFlow Freeze layers <code>requires_grad = False</code> <code>layer.trainable = False</code> Replace output layer <code>model.fc = nn.Linear(...)</code> <code>Dense(...)</code> on top of <code>base_model</code> Adaptive pooling <code>nn.AdaptiveAvgPool2d((1,1))</code> <code>GlobalAveragePooling2D()</code> Weight decay <code>optimizer(..., weight_decay=...)</code> <code>kernel_regularizer=l2(...)</code> Class weighting <code>CrossEntropyLoss(weight=...)</code> <code>class_weight={...}</code> in <code>fit()</code> Early stopping Manual or custom Built-in <code>EarlyStopping</code> callback Gradual unfreeze Manual per parameter Manual per layer"},{"location":"chapter15/#mini-exercise","title":"Mini-Exercise","text":"<p>Fine-tune a pretrained ResNet50 to classify 3 new classes.</p> <ol> <li>Load model with <code>include_top=False</code></li> <li> <p>Add:</p> </li> <li> <p>Global average pooling</p> </li> <li>Dense output layer</li> <li>Freeze base</li> <li>Train only top for 5 epochs</li> <li>Then unfreeze last block</li> <li> <p>Add:</p> </li> <li> <p>Dropout</p> </li> <li>L2 regularization</li> <li>ReduceLROnPlateau</li> <li>Early stopping</li> <li>Plot train/val loss and accuracy</li> <li>Identify if it overfits or underfits</li> </ol> <p>Bonus: Try with both PyTorch and TensorFlow.</p>"},{"location":"chapter16/","title":"Chapter 16: Train vs Eval Mode","text":"<p>\u201cA model is like a chameleon\u2014it changes behavior depending on whether it\u2019s training or being tested. Know when it\u2019s learning and when it should just perform.\u201d</p>"},{"location":"chapter16/#why-this-chapter-matters","title":"Why This Chapter Matters","text":"<p>When you're debugging or deploying CNNs, switching between training and evaluation (inference) modes is crucial. Two components in particular behave differently depending on the mode:</p> <ul> <li>Dropout: Randomly disables neurons during training to prevent overfitting</li> <li>Batch Normalization: Uses running mean/variance during inference instead of batch statistics</li> </ul> <p>If you forget to switch to inference mode:</p> <ul> <li>The model will behave unpredictably</li> <li>Validation accuracy will look unstable</li> <li>Final test performance may drop drastically</li> </ul> <p>This chapter helps you:</p> <ul> <li>Understand the mechanics of mode switching</li> <li>Avoid silent bugs in inference</li> <li>Use PyTorch and TensorFlow's tools for correct evaluation behavior</li> </ul>"},{"location":"chapter16/#conceptual-breakdown","title":"Conceptual Breakdown","text":""},{"location":"chapter16/#the-two-modes","title":"\ud83d\udd39 The Two Modes","text":"Mode Description When to Use Train Active learning: dropout + batchnorm use live data During training phase Eval Inference mode: deterministic behavior During validation, testing, or deployment"},{"location":"chapter16/#layers-that-behave-differently","title":"\ud83d\udd39 Layers That Behave Differently","text":"Layer Type Training Mode Behavior Eval Mode Behavior Dropout Randomly zeroes out neurons per batch Skipped entirely (no dropout at inference) BatchNorm Uses current batch stats for normalization Uses running (moving average) stats"},{"location":"chapter16/#pytorch-implementation","title":"PyTorch Implementation","text":""},{"location":"chapter16/#switching-modes","title":"\ud83d\udd38 Switching Modes","text":"<pre><code>model.train()  # Activates Dropout + BatchNorm (training mode)\n...\nmodel.eval()   # Freezes Dropout + BatchNorm (inference mode)\n</code></pre>"},{"location":"chapter16/#disabling-gradients-during-inference","title":"\ud83d\udd38 Disabling Gradients During Inference","text":"<pre><code>model.eval()\nwith torch.no_grad():\n    outputs = model(images)\n</code></pre> <p>Why <code>torch.no_grad()</code>?</p> <ul> <li>Saves memory</li> <li>Speeds up inference</li> <li>Ensures gradients are not computed (and no backward graph is tracked)</li> </ul>"},{"location":"chapter16/#validation-code-snippet","title":"\ud83d\udd38 Validation Code Snippet","text":"<pre><code>model.eval()\nval_loss = 0.0\ncorrect = 0\n\nwith torch.no_grad():\n    for images, labels in val_loader:\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        val_loss += loss.item()\n        pred = outputs.argmax(dim=1)\n        correct += pred.eq(labels).sum().item()\n</code></pre>"},{"location":"chapter16/#tensorflow-implementation","title":"TensorFlow Implementation","text":""},{"location":"chapter16/#mode-switching-with-trainingtruefalse","title":"\ud83d\udd38 Mode Switching with <code>training=True/False</code>","text":"<p>In TensorFlow, mode is passed explicitly to the model during <code>call()</code>:</p> <pre><code>preds = model(images, training=True)   # Train mode\npreds = model(images, training=False)  # Inference mode\n</code></pre> <p>You must use this flag correctly in:</p> <ul> <li>Manual training loops</li> <li>Custom <code>Model</code> subclasses</li> </ul>"},{"location":"chapter16/#example-manual-validation","title":"\ud83d\udd38 Example: Manual Validation","text":"<pre><code># Validation step\nfor x_batch_val, y_batch_val in val_dataset:\n    val_logits = model(x_batch_val, training=False)  # eval mode\n    val_loss = loss_fn(y_batch_val, val_logits)\n</code></pre>"},{"location":"chapter16/#with-modelfit","title":"\ud83d\udd38 With <code>model.fit()</code>","text":"<p>Keras handles mode switching automatically when using <code>model.fit()</code> and <code>model.evaluate()</code>. But in custom training loops, it\u2019s manual.</p>"},{"location":"chapter16/#model-summary-check","title":"\ud83d\udd38 Model Summary Check","text":"<pre><code>model.summary()  # Always available\nprint([layer.trainable for layer in model.layers])  # Check trainable flags\n</code></pre>"},{"location":"chapter16/#common-mistakes-and-how-to-avoid-them","title":"Common Mistakes and How to Avoid Them","text":"Mistake Consequence Fix Forgetting <code>.eval()</code> in PyTorch Dropout and BN are active during inference Call <code>model.eval()</code> before validation or testing Forgetting <code>training=False</code> in TensorFlow Model behaves like it's still training Pass <code>training=False</code> explicitly in calls Not using <code>torch.no_grad()</code> Higher memory usage during inference Wrap evaluation in <code>with torch.no_grad()</code> Logging wrong metrics Misinterpreted validation accuracy Ensure eval mode + no gradient during val"},{"location":"chapter16/#framework-comparison-table","title":"Framework Comparison Table","text":"Concept PyTorch TensorFlow Activate training mode <code>model.train()</code> <code>training=True</code> in <code>model(x, training=True)</code> Activate evaluation mode <code>model.eval()</code> <code>training=False</code> Disable gradient tracking <code>with torch.no_grad():</code> Automatic in <code>fit()</code> / use tape manually BatchNorm/Dropout behavior Respects mode setting Respects <code>training</code> flag Manual control needed Yes Yes for custom loops, no for <code>fit()</code>"},{"location":"chapter16/#mini-exercise","title":"Mini-Exercise","text":"<p>Implement the following for a simple CNN classifier:</p> <ol> <li>Write a validation loop in both PyTorch and TensorFlow</li> <li>In PyTorch, explicitly switch between <code>.train()</code> and <code>.eval()</code>, and use <code>torch.no_grad()</code></li> <li>In TensorFlow, pass <code>training=True</code> or <code>False</code> depending on the phase</li> <li>Compare the model output with dropout active vs inactive</li> </ol> <p>Bonus: Log memory usage during inference with and without gradient tracking</p>"},{"location":"chapter16/#what-you-can-now-do","title":"What You Can Now Do","text":"<ul> <li>Evaluate models with consistent accuracy and no randomness</li> <li>Avoid common dropout and batchnorm bugs</li> <li> <p>Use inference mode to:</p> </li> <li> <p>Save memory</p> </li> <li>Improve speed</li> <li>Ensure deployment stability</li> </ul>"},{"location":"chapter17/","title":"Chapter 17: Visualizing Feature Maps and Filters","text":"<p>\u201cCNNs are not black boxes. You just need the right lens to peek inside.\u201d</p>"},{"location":"chapter17/#why-this-chapter-matters","title":"Why This Chapter Matters","text":"<p>As your models become more complex, the need for interpretability becomes critical:</p> <ul> <li>Are filters detecting edges, textures, or irrelevant noise?</li> <li>Why did my CNN misclassify an image?</li> <li>Is it paying attention to the right part of the image?</li> </ul> <p>Feature visualization answers these by revealing:</p> <ul> <li>What each layer activates on</li> <li>How filters evolve with training</li> <li>Whether attention focuses on objects or noise</li> </ul> <p>This chapter teaches:</p> <ul> <li>How to extract intermediate outputs (feature maps)</li> <li>Visualize filters learned by convolutional layers</li> <li>Build debugging tools for visual interpretation</li> </ul>"},{"location":"chapter17/#conceptual-breakdown","title":"Conceptual Breakdown","text":""},{"location":"chapter17/#what-are-feature-maps","title":"\ud83d\udd39 What Are Feature Maps?","text":"<p>A feature map is the output of a convolution layer:</p> <ul> <li>It shows activation patterns for a given image</li> <li>Early maps detect edges, textures</li> <li>Deeper maps focus on shapes, semantics</li> </ul> <p>A single input image generates many feature maps, one for each filter (channel) in the layer.</p>"},{"location":"chapter17/#what-are-filters","title":"\ud83d\udd39 What Are Filters?","text":"<p>Filters are the learnable weight matrices applied during convolution:</p> <ul> <li>Size: typically 3\u00d73 or 5\u00d75</li> <li>Shape: <code>[out_channels, in_channels, kernel_size, kernel_size]</code></li> </ul> <p>Visualizing filters helps you:</p> <ul> <li>Understand learned patterns</li> <li>Identify dead or redundant filters</li> <li>Compare pretrained vs randomly initialized filters</li> </ul>"},{"location":"chapter17/#pytorch-implementation","title":"PyTorch Implementation","text":""},{"location":"chapter17/#1-visualizing-intermediate-feature-maps","title":"\ud83d\udd38 1. Visualizing Intermediate Feature Maps","text":"<p>Let\u2019s visualize the output after the first convolutional block.</p>"},{"location":"chapter17/#step-1-register-forward-hook","title":"Step 1: Register Forward Hook","text":"<pre><code>activations = {}\n\ndef hook_fn(module, input, output):\n    activations['conv1'] = output.detach()\n\nmodel.conv1.register_forward_hook(hook_fn)\n</code></pre>"},{"location":"chapter17/#step-2-forward-pass-an-image","title":"Step 2: Forward Pass an Image","text":"<pre><code>model.eval()\nwith torch.no_grad():\n    _ = model(input_image.unsqueeze(0))  # Batch of 1\n</code></pre>"},{"location":"chapter17/#step-3-visualize-the-activation-maps","title":"Step 3: Visualize the Activation Maps","text":"<pre><code>import matplotlib.pyplot as plt\n\nact = activations['conv1'].squeeze()  # shape: [num_filters, H, W]\n\nfig, axes = plt.subplots(4, 8, figsize=(12, 6))  # for 32 filters\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(act[i].cpu(), cmap='viridis')\n    ax.axis('off')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"chapter17/#2-visualizing-filters","title":"\ud83d\udd38 2. Visualizing Filters","text":"<pre><code>filters = model.conv1.weight.data.clone()  # shape: [out_channels, in_channels, k, k]\n\nfig, axes = plt.subplots(4, 8, figsize=(12, 6))\nfor i, ax in enumerate(axes.flat):\n    filt = filters[i, 0]  # visualize first channel of each filter\n    ax.imshow(filt.cpu(), cmap='gray')\n    ax.axis('off')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"chapter17/#tensorflow-implementation","title":"TensorFlow Implementation","text":""},{"location":"chapter17/#1-define-sub-model-for-intermediate-layers","title":"\ud83d\udd38 1. Define Sub-Model for Intermediate Layers","text":"<p>Keras makes this straightforward:</p> <pre><code>from tensorflow.keras.models import Model\n\nlayer_outputs = [layer.output for layer in model.layers if 'conv' in layer.name]\nactivation_model = Model(inputs=model.input, outputs=layer_outputs)\n</code></pre>"},{"location":"chapter17/#2-forward-pass-and-visualize","title":"\ud83d\udd38 2. Forward Pass and Visualize","text":"<pre><code>activations = activation_model.predict(input_image[None, ...])  # Add batch dim\n\nfirst_layer_activations = activations[0]  # shape: [1, H, W, filters]\n\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(4, 8, figsize=(12, 6))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(first_layer_activations[0, :, :, i], cmap='viridis')\n    ax.axis('off')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"chapter17/#3-visualize-filters-of-a-conv-layer","title":"\ud83d\udd38 3. Visualize Filters of a Conv Layer","text":"<pre><code>weights = model.get_layer('conv2d').get_weights()[0]  # shape: (k, k, in_channels, out_channels)\n\nfig, axes = plt.subplots(4, 8, figsize=(12, 6))\nfor i, ax in enumerate(axes.flat):\n    filt = weights[:, :, 0, i]  # visualizing first input channel\n    ax.imshow(filt, cmap='gray')\n    ax.axis('off')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"chapter17/#framework-comparison-table","title":"Framework Comparison Table","text":"Task PyTorch TensorFlow/Keras Extract feature map Use forward hook Use <code>Model(inputs, outputs)</code> Visualize filter weights Access <code>layer.weight.data</code> <code>layer.get_weights()</code> View multiple layers Register multiple hooks Output list from sub-model Forward inference <code>with torch.no_grad()</code> <code>model.predict()</code> or <code>model(x, training=False)</code> Activation shape <code>[batch, channels, H, W]</code> <code>[batch, H, W, channels]</code>"},{"location":"chapter17/#mini-exercise","title":"Mini-Exercise","text":"<ol> <li>Choose any pretrained model (ResNet18, MobileNet, etc.)</li> <li>Input a single image of a dog or cat</li> <li> <p>Extract and visualize:</p> </li> <li> <p>Filters from the first Conv2D layer</p> </li> <li>Activation maps from the first 2 convolutional blocks</li> <li> <p>Interpret:</p> </li> <li> <p>Which filters activate strongest?</p> </li> <li>What kind of patterns do they seem to detect?</li> <li> <p>Bonus:</p> </li> <li> <p>Try with a different image (car, flower) and compare changes</p> </li> </ol>"},{"location":"chapter17/#debugging-insights","title":"Debugging Insights","text":"<p>Visualizations help you discover:</p> <ul> <li>Dead filters: filters with near-zero activation across samples</li> <li>Bias: model focusing on background rather than subject</li> <li>Overfitting: filters overly tuned to irrelevant details</li> </ul> <p>You can also visualize wrong predictions to understand why your model failed.</p>"},{"location":"chapter17/#what-you-can-now-do","title":"What You Can Now Do","text":"<ul> <li>Open the black box of your CNN models</li> <li>Debug incorrect outputs by checking what the model \u201csees\u201d</li> <li>Inspect learned filters to ensure diversity and relevance</li> <li>Use visualization as a sanity check during training</li> </ul>"},{"location":"chapter18/","title":"Chapter 18: Inference Pipeline Design","text":"<p>\u201cTraining wins the accuracy race. Inference wins the deployment game.\u201d</p>"},{"location":"chapter18/#why-this-chapter-matters","title":"Why This Chapter Matters","text":"<p>No matter how well your model performs in training, it\u2019s useless if it fails at inference time.</p> <p>Here\u2019s where things often go wrong:</p> <ul> <li>You trained with <code>[0,1]</code> scaled images but used <code>[-1,1]</code> scaling at inference</li> <li>You forgot to switch to <code>.eval()</code> mode or <code>training=False</code></li> <li>You tested with images of a different size than during training</li> <li>Your real-world images have noise, padding, or background not present in your dataset</li> </ul> <p>Inference is a system, not just a <code>.predict()</code> call.</p> <p>This chapter shows you how to:</p> <ul> <li>Design reusable, consistent, and fault-tolerant pipelines</li> <li>Align preprocessing between training and deployment</li> <li>Apply test-time augmentation (TTA) for better accuracy</li> <li>Defend your model against bad input or unexpected formats</li> </ul>"},{"location":"chapter18/#conceptual-breakdown","title":"Conceptual Breakdown","text":""},{"location":"chapter18/#what-is-an-inference-pipeline","title":"\ud83d\udd39 What is an Inference Pipeline?","text":"<p>It\u2019s the entire path an image takes from user input to model prediction:</p> <ol> <li>Image is uploaded, captured, or streamed</li> <li>Preprocessing (resize, normalize, etc.)</li> <li>Passed through model (in eval mode, no gradients)</li> <li>Output is decoded (softmax, argmax, etc.)</li> <li>Results are returned in user-friendly format</li> </ol> <p>A mistake at any step will lead to wrong predictions.</p>"},{"location":"chapter18/#training-vs-inference-matching-pipelines","title":"\ud83d\udd39 Training vs Inference: Matching Pipelines","text":"Stage During Training During Inference Resize <code>Resize((224, 224))</code> Same exact shape required Normalization <code>Normalize(mean, std)</code> or <code>rescale to [0,1]</code> Must match exactly Augmentations RandomCrop, Flip, ColorJitter (for variety) Disabled, or TTA only Mode <code>model.train()</code> <code>model.eval()</code> Gradients <code>requires_grad=True</code> <code>no_grad()</code> / tape disabled <p>If you change any of the above during inference, your model may misbehave.</p>"},{"location":"chapter18/#pytorch-implementation","title":"PyTorch Implementation","text":""},{"location":"chapter18/#1-reusable-preprocessing-function","title":"\ud83d\udd38 1. Reusable Preprocessing Function","text":"<pre><code>from torchvision import transforms\n\ndef get_inference_transform():\n    return transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])  # Must match training!\n    ])\n</code></pre>"},{"location":"chapter18/#2-inference-function","title":"\ud83d\udd38 2. Inference Function","text":"<pre><code>from PIL import Image\nimport torch\n\ndef predict_image(model, image_path, transform):\n    model.eval()\n    image = Image.open(image_path).convert(\"RGB\")\n    tensor = transform(image).unsqueeze(0)  # Add batch dimension\n\n    with torch.no_grad():\n        output = model(tensor)\n        prediction = torch.argmax(output, dim=1).item()\n    return prediction\n</code></pre>"},{"location":"chapter18/#tensorflow-implementation","title":"TensorFlow Implementation","text":""},{"location":"chapter18/#1-preprocessing-function","title":"\ud83d\udd38 1. Preprocessing Function","text":"<p>For models like MobileNet or EfficientNet, use built-in preprocessors:</p> <pre><code>from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\nfrom tensorflow.keras.preprocessing import image\nimport numpy as np\n\ndef prepare_image_tf(img_path):\n    img = image.load_img(img_path, target_size=(224, 224))\n    img_array = image.img_to_array(img)\n    img_array = preprocess_input(img_array)  # Handles [-1,1] scaling\n    return np.expand_dims(img_array, axis=0)\n</code></pre>"},{"location":"chapter18/#2-inference-function_1","title":"\ud83d\udd38 2. Inference Function","text":"<pre><code>def predict_tf(model, img_path):\n    img_tensor = prepare_image_tf(img_path)\n    predictions = model.predict(img_tensor)\n    return np.argmax(predictions)\n</code></pre>"},{"location":"chapter18/#additions-for-a-full-inference-system","title":"Additions for a Full Inference System","text":"Component Why It Matters Input validation Ensure correct shape, color channels Test-Time Augmentation Improve prediction by averaging outputs Softmax thresholding Avoid low-confidence predictions Postprocessing Map label index \u2192 human-readable class Batching Speed up inference for multiple inputs"},{"location":"chapter18/#optional-test-time-augmentation-tta","title":"\ud83d\udd38 Optional: Test-Time Augmentation (TTA)","text":"<p>Run multiple variants of the same image and average predictions.</p> <pre><code>def tta_predict(model, image, transforms_list):\n    outputs = []\n    for t in transforms_list:\n        img = t(image).unsqueeze(0)\n        with torch.no_grad():\n            output = model(img)\n        outputs.append(output)\n    return torch.stack(outputs).mean(dim=0).argmax().item()\n</code></pre>"},{"location":"chapter18/#framework-comparison-table","title":"Framework Comparison Table","text":"Feature PyTorch TensorFlow / Keras Eval mode <code>model.eval()</code> <code>training=False</code> in <code>model(x, training=...)</code> Gradient-free inference <code>with torch.no_grad()</code> Default in <code>model.predict()</code> Reusable preprocessing <code>torchvision.transforms.Compose()</code> <code>keras.preprocessing</code> or <code>tf.image</code> Built-in TTA Manual Manual Model saving <code>torch.save(model.state_dict())</code> <code>model.save()</code> to SavedModel format Normalization consistency User-defined Use <code>keras.applications.*.preprocess_input()</code>"},{"location":"chapter18/#mini-exercise","title":"Mini-Exercise","text":"<p>Build a full inference function that:</p> <ol> <li>Accepts an image path</li> <li>Applies identical preprocessing from training</li> <li>Loads a trained model</li> <li>Switches to inference mode</li> <li>Predicts the class and returns a human-readable label</li> </ol> <p>Bonus:</p> <ul> <li>Add test-time augmentation</li> <li>Log input/output shape and prediction confidence</li> </ul>"},{"location":"chapter18/#gotchas-to-watch-out-for","title":"Gotchas to Watch Out For","text":"Problem Likely Cause Model always predicts same class Forgetting <code>.eval()</code> or bad normalization High training accuracy, poor test Mismatched preprocessing (e.g., RGB to BGR) Inference crashes on large input Missing batch dimension or wrong shape Weird predictions at deployment Dropout still active, or inconsistent mode"},{"location":"chapter18/#what-you-can-now-do","title":"What You Can Now Do","text":"<ul> <li>Write a robust inference script from scratch</li> <li>Detect input shape and channel mismatches</li> <li>Reuse training transforms to guarantee consistency</li> <li>Use test-time augmentation to improve generalization</li> <li>Ship CNNs in reproducible, traceable pipelines</li> </ul>"},{"location":"chapter19/","title":"Chapter 19: Common Errors and How to Debug Them","text":"<p>\u201cThe most dangerous bugs are silent. CNNs won\u2019t throw exceptions when they\u2019re wrong\u2014they\u2019ll just smile and mispredict.\u201d</p>"},{"location":"chapter19/#why-this-chapter-matters","title":"Why This Chapter Matters","text":"<p>After training your CNN and setting up a beautiful inference pipeline, everything looks good\u2014but:</p> <ul> <li>The model always predicts the same class</li> <li>Accuracy is much lower than expected</li> <li>It fails on real-world images even if training went fine</li> </ul> <p>These are not training bugs. They\u2019re systemic failures often due to:</p> <ul> <li>Data leakage</li> <li>Input misalignment</li> <li>Normalization errors</li> <li>Inconsistent shapes</li> <li>Forgotten mode switching</li> </ul> <p>This chapter equips you with a checklist and mindset to debug confidently.</p>"},{"location":"chapter19/#conceptual-breakdown","title":"Conceptual Breakdown","text":""},{"location":"chapter19/#where-most-bugs-hide","title":"\ud83d\udd0d Where Most Bugs Hide","text":"Bug Type Example Detection Strategy Normalization Wrong mean/std or none at inference Compare input histograms before and after Shape mismatch Model expects [3, 224, 224], gets [1, 256, 256] Print <code>.shape</code> at each step Wrong eval mode Model trained well but fails validation Ensure <code>model.eval()</code> or <code>training=False</code> Data leakage Same image appears in train and test set Check file paths, cross-fold splits One-class output Predicts class 0 for everything Check for class imbalance, final layer, softmax use"},{"location":"chapter19/#pytorch-debugging-checklist","title":"PyTorch Debugging Checklist","text":"<pre><code># 1. Check model mode\nprint(model.training)  # Should be False during inference\n\n# 2. Check input shape\nprint(image_tensor.shape)  # Should be [1, 3, 224, 224]\n\n# 3. Check normalization\nprint(image_tensor.min(), image_tensor.max())  # Should be ~-1 to 1\n\n# 4. Visual sanity check\nimport matplotlib.pyplot as plt\nplt.imshow(image_tensor.squeeze().permute(1, 2, 0).numpy())\n</code></pre>"},{"location":"chapter19/#tensorflow-debugging-checklist","title":"TensorFlow Debugging Checklist","text":"<pre><code># 1. Model mode\nlogits = model(img, training=False)\n\n# 2. Shape check\nprint(img.shape)  # Should be (1, 224, 224, 3)\n\n# 3. Normalization\nprint(img.min().numpy(), img.max().numpy())  # Depending on preprocess_input()\n\n# 4. Show input\nplt.imshow(img[0] / 2 + 0.5)  # Undo [-1,1] normalization for visualization\n</code></pre>"},{"location":"chapter19/#common-cnn-errors-fixes","title":"\ud83d\udee0 Common CNN Errors &amp; Fixes","text":"Symptom Cause Fix Always predicting one class Class imbalance, untrained head, no softmax Use weighted loss, check class distribution, inspect logits Very low accuracy at inference Wrong normalization Match training transforms exactly Crash on inference Missing batch dim or float32 type Use <code>.unsqueeze(0)</code> and convert to <code>float32</code> Fails on grayscale / RGBA image Expecting 3-channel RGB Convert image <code>.convert(\"RGB\")</code> Prediction unstable during validation Forgot <code>.eval()</code> or <code>no_grad()</code> Call <code>model.eval()</code> and use <code>torch.no_grad()</code> Shape mismatch in pretrained model Mismatch in input resolution or output class count Resize input and adapt output layer Test image looks weird Bad rescale or wrong channel order Visualize before and after transforms Weird outputs in browser/mobile Tensor converted incorrectly to JS or HTML format Normalize properly and ensure channels/byte values are valid"},{"location":"chapter19/#visual-debugging-techniques","title":"Visual Debugging Techniques","text":""},{"location":"chapter19/#1-visualize-inputs-before-and-after-transform","title":"1. Visualize Inputs Before and After Transform","text":"<pre><code># PyTorch\nplt.subplot(1,2,1); plt.imshow(PIL_img)\nplt.subplot(1,2,2); plt.imshow(tensor.permute(1, 2, 0).numpy())\n\n# TensorFlow\nplt.imshow(img_tensor[0] / 2 + 0.5)  # If normalized to [-1, 1]\n</code></pre>"},{"location":"chapter19/#2-log-confidence-scores","title":"2. Log Confidence Scores","text":"<pre><code># PyTorch\nprobs = torch.softmax(outputs, dim=1)\nprint(probs.topk(3))  # Top-3 prediction scores\n\n# TensorFlow\nprobs = tf.nn.softmax(preds).numpy()\nprint(np.argsort(probs[0])[-3:][::-1])  # Top-3 classes\n</code></pre>"},{"location":"chapter19/#3-debug-batch-processing","title":"3. Debug Batch Processing","text":"<ul> <li>Ensure same shape per sample</li> <li>All batches should be same dtype</li> <li>Check shuffling order</li> </ul>"},{"location":"chapter19/#defensive-programming-tips","title":"Defensive Programming Tips","text":"Strategy Benefit <code>assert image.shape == (1, 3, 224, 224)</code> Prevents hidden shape bugs <code>assert img.dtype == torch.float32</code> Ensures model receives valid input Logging before/after each step Makes silent bugs visible Try inference on one image manually Removes complexity, isolates the problem Unit test your transforms Catch errors early"},{"location":"chapter19/#framework-comparison-table","title":"Framework Comparison Table","text":"Debug Task PyTorch TensorFlow / Keras Check training/eval mode <code>model.training</code> Explicit <code>training=False</code> flag Visualize input <code>permute(1,2,0)</code> on tensor <code>/ 2 + 0.5</code> if normalized to [-1,1] Print prediction scores <code>softmax(outputs, dim=1)</code> <code>tf.nn.softmax()</code> One-image inference <code>unsqueeze(0)</code> and <code>no_grad()</code> <code>np.expand_dims()</code> and <code>model.predict()</code> Inspect model layers <code>print(model)</code> or <code>summary()</code> <code>model.summary()</code>"},{"location":"chapter19/#mini-exercise","title":"Mini-Exercise","text":"<p>Try this on your own CNN project:</p> <ol> <li>Pick an image from your test set</li> <li>Run it through your full pipeline</li> <li> <p>Log:</p> </li> <li> <p>Input shape, dtype</p> </li> <li>Image pixel range before/after preprocessing</li> <li>Top-3 predicted classes with confidence</li> <li> <p>Visualize:</p> </li> <li> <p>Input image</p> </li> <li>Preprocessed tensor</li> <li>Activation maps (from Chapter 17)</li> <li> <p>Bonus:</p> </li> <li> <p>Temporarily insert an invalid image (grayscale, wrong shape) and handle it gracefully</p> </li> </ol>"},{"location":"chapter19/#final-tips-the-debugging-mindset","title":"\ud83d\udd1a Final Tips: The Debugging Mindset","text":"<ul> <li>Assume nothing\u2014even if training went perfectly</li> <li>Print and plot often</li> <li>Start small: one image, one batch</li> <li>Compare to known-good outputs (reference image \u2192 known prediction)</li> <li>Trace the full pipeline: from raw input to final prediction</li> </ul>"},{"location":"chapter19/#what-you-can-now-do","title":"What You Can Now Do","text":"<ul> <li>Build sanity-checked pipelines that won\u2019t fail silently</li> <li>Fix models that seem broken but are just misconfigured</li> <li>Gain confidence and trust in your model\u2019s predictions</li> <li>Catch and fix systemic bugs before they go live</li> </ul>"},{"location":"chapter2/","title":"Chapter 1: What is a Tensor (in Code and in Mind)?","text":"<p>\u201cYou can\u2019t debug what you don\u2019t understand. And in deep learning, most confusion begins with shapes.\u201d</p>"},{"location":"chapter2/#why-this-chapter-matters","title":"Why This Chapter Matters","text":"<p>Before we train any CNN, before we pass an image through any layer, there\u2019s something more foundational: the tensor. It\u2019s not just a data structure\u2014it\u2019s the very language your model thinks in.</p> <p>You might know it as just a NumPy array or a multi-dimensional grid. But in computer vision, a tensor carries the shape of your reality. Understanding how to think in tensors is what separates beginners who get stuck at shape mismatches\u2026 from engineers who can flow seamlessly between [C, H, W] and [H, W, C], across any framework.</p> <p>In this chapter, we\u2019ll demystify tensors\u2014not just how to use them in PyTorch and TensorFlow, but how to think about them in your head. We\u2019ll learn to reshape, permute, slice, and batch like second nature.</p>"},{"location":"chapter2/#conceptual-breakdown","title":"Conceptual Breakdown","text":""},{"location":"chapter2/#what-is-a-tensor","title":"\ud83d\udd39 What is a tensor?","text":"<p>A tensor is a generalization of scalars, vectors, and matrices into higher dimensions.</p> <ul> <li> <p>A scalar is a 0D tensor: 5</p> </li> <li> <p>A vector is a 1D tensor: [5, 3, 2]</p> </li> <li> <p>A matrix is a 2D tensor: [[1, 2], [3, 4]]</p> </li> <li> <p>An image is typically a 3D tensor: Channels \u00d7 Height \u00d7 Width or Height \u00d7 Width \u00d7 Channels</p> </li> <li> <p>A batch of images? A 4D tensor.</p> </li> </ul> <p>So, for example: <pre><code>A grayscale image (28x28) \u2192 [28, 28]  \nAn RGB image (224x224) \u2192 [3, 224, 224] or [224, 224, 3]  \nA batch of RGB images (32 images) \u2192 [32, 3, 224, 224] or [32, 224, 224, 3]\n</code></pre> Tensors store both the data and the shape. Understanding and controlling that shape is critical.</p>"},{"location":"chapter2/#pytorch-implementation","title":"PyTorch Implementation","text":"<p>In PyTorch, tensors are created and manipulated using <code>torch.tensor</code>, and reshaped with <code>.view()</code>, .<code>reshape()</code>, and <code>.permute()</code>.</p> <p>\ud83d\udd38 Basic Creation</p> <pre><code>import torch\n\nscalar = torch.tensor(3)  # 0D\nvector = torch.tensor([1, 2, 3])  # 1D\nmatrix = torch.tensor([[1, 2], [3, 4]])  # 2D\nimage = torch.rand(3, 224, 224)  # 3D image tensor (C, H, W)\n</code></pre> <p>\ud83d\udd38 Reshaping and Permuting</p> <pre><code># Reshape: flatten a tensor\nimage_flat = image.view(-1)  # Total elements = 3*224*224\n\n# Permute: switch dimensions\nimage_hw_c = image.permute(1, 2, 0)  # Now (H, W, C)\n\n# Add batch dimension\nimage_batch = image.unsqueeze(0)  # (1, 3, 224, 224)\n</code></pre>"},{"location":"chapter2/#tensorflow-implementation","title":"TensorFlow Implementation","text":"<p>In TensorFlow, we use <code>tf.constant</code>, <code>tf.reshape()</code>, and <code>tf.transpose()</code>.</p> <p>\ud83d\udd38 Basic Creation <pre><code>import tensorflow as tf\n\nscalar = tf.constant(3)  # 0D\nvector = tf.constant([1, 2, 3])  # 1D\nmatrix = tf.constant([[1, 2], [3, 4]])  # 2D\nimage = tf.random.uniform((224, 224, 3))  # (H, W, C)\n</code></pre></p> <p>\ud83d\udd38 Reshaping and Transposing</p> <pre><code># Flatten\nimage_flat = tf.reshape(image, [-1])\n\n# Transpose: (H, W, C) \u2192 (C, H, W)\nimage_chw = tf.transpose(image, [2, 0, 1])\n\n# Add batch dimension\nimage_batch = tf.expand_dims(image, axis=0)  # (1, 224, 224, 3)\n</code></pre>"},{"location":"chapter2/#framework-comparison-table","title":"Framework Comparison Table","text":"Concept PyTorch TensorFlow Tensor class torch.Tensor tf.Tensor Shape format (image) [C, H, W] [H, W, C] Reshape .view(), .reshape() tf.reshape() Transpose / Permute .permute(dim_order) tf.transpose(dim_order) Add batch dimension .unsqueeze(dim) tf.expand_dims(tensor, axis) Flatten .view(-1) tf.reshape(tensor, [-1])"},{"location":"chapter2/#mini-exercise","title":"Mini-Exercise","text":"<p>Try loading a single image and perform the following in both PyTorch and TensorFlow:</p> <ol> <li> <p>Load the image as an array (e.g., with PIL or <code>tf.io</code>)</p> </li> <li> <p>Convert it to a tensor</p> </li> <li> <p>Normalize the pixel values to <code>[0, 1]</code></p> </li> <li> <p>Add a batch dimension</p> </li> <li> <p>Ensure the shape is correct for your framework (PyTorch: <code>[1, 3, H, W]</code>, TF: <code>[1, H, W, 3]</code>)</p> </li> </ol>"},{"location":"chapter3/","title":"Chapter 3: From Pixels to Model Input","text":"<p>\u201cYour model is only as good as the input you feed it. Garbage in, garbage out\u2014but beautifully preprocessed data in? That\u2019s how deep learning begins.\u201d</p>"},{"location":"chapter3/#why-this-chapter-matters","title":"Why This Chapter Matters","text":"<p>At this point, you understand how images are stored and how to manipulate tensors. Now we take the next step: building a complete, robust input pipeline that takes an image from file system \u2192 tensor \u2192 model-ready format.</p> <p>This chapter answers:</p> <ul> <li> <p>How do you convert raw image data to a float32 tensor?</p> </li> <li> <p>What\u2019s the difference between resizing and reshaping?</p> </li> <li> <p>Why do batch dimensions matter?</p> </li> <li> <p>What happens when you feed data into a Conv2D or Dense layer?</p> </li> <li> <p>How do PyTorch and TensorFlow differ in handling the image input flow?</p> </li> </ul> <p>Whether you're loading a single image for inference or setting up batches for training, this chapter will help you debug shape mismatches, clean up input pipelines, and feed data correctly into your network.</p>"},{"location":"chapter3/#conceptual-breakdown","title":"Conceptual Breakdown","text":"<p>\ud83d\udd39 Full Image Input Pipeline Overview</p> <p>Every image input to a CNN passes through a pipeline like this: <pre><code>File (JPEG/PNG)\n \u2193\nLoad to memory (PIL / tf.io / OpenCV)\n \u2193\nConvert to RGB (if not already)\n \u2193\nResize or reshape to match model expectations\n \u2193\nConvert to float32\n \u2193\nNormalize (0\u20131 or mean/std)\n \u2193\nReorder dimensions if needed ([H, W, C] \u2194 [C, H, W])\n \u2193\nAdd batch dimension\n \u2193\nFeed to CNN layer\n</code></pre></p> <p>This process must be precise, especially when you're working with pretrained models or initializing new architectures.</p> <p>\ud83d\udd39 Resize vs Reshape</p> <p>Understanding this difference is critical.</p> <ul> <li> <p>Resize changes the actual content dimensions (resampling, possibly distorting image slightly).</p> </li> <li> <p>Example: resize 640\u00d7480 \u2192 224\u00d7224</p> </li> <li> <p>Reshape changes the data layout without touching the content. Dangerous if shape is wrong!</p> </li> <li> <p>Only use reshape if you're 100% sure of data layout.</p> </li> </ul> <p>\ud83d\udccc Resizing is typically used for image preprocessing. Reshape is for tensor manipulation post-preprocessing.</p> <p>\ud83d\udd39 Normalization: Why Float32 and 0\u20131?</p> <p>CNNs expect normalized input:</p> <ul> <li> <p>Pixel values from 0\u2013255 are too large and make training unstable.</p> </li> <li> <p>Convert to float32 (<code>/255.0</code>) or apply dataset-specific mean-std normalization.</p> </li> </ul> <p>Common norms:</p> <ul> <li> <p><code>[0.0, 1.0]</code> scaling \u2192 generic models</p> </li> <li> <p><code>mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]</code> \u2192 ImageNet-pretrained models</p> </li> </ul> <p>\ud83d\udd39 Batch Dimension: Don\u2019t Forget!</p> <p>Even for one image, CNNs expect a batch:</p> <ul> <li> <p>Conv2D: Expects input shape <code>[N, C, H, W]</code> (PyTorch) or <code>[N, H, W, C]</code> (TF)</p> </li> <li> <p>N is batch size: must be \u22651</p> </li> <li> <p>Failing to add this leads to shape errors when feeding into models</p> </li> </ul> <p>\ud83d\udccc Use <code>.unsqueeze(0)</code> (PyTorch) or <code>tf.expand_dims(..., axis=0)</code> (TensorFlow)</p> <p>\ud83d\udd39 Feeding Into a Conv2D or Dense Layer</p> <p>CNNs process 4D tensors:</p> <ul> <li> <p>PyTorch: <code>[batch, channels, height, width]</code></p> </li> <li> <p>TensorFlow: <code>[batch, height, width, channels]</code></p> </li> </ul> <p>What happens internally:</p> <ul> <li> <p>Conv2D takes a window of pixels</p> </li> <li> <p>Applies filters (kernels)</p> </li> <li> <p>Outputs a feature map</p> </li> <li> <p>The deeper the layers, the higher the abstraction</p> </li> </ul> <p>Dense layers flatten the features:</p> <ul> <li> <p>Input must be reshaped before connecting to <code>nn.Linear</code> or <code>Dense()</code></p> </li> <li> <p>Usually done with <code>.view(batch_size, -1)</code> or <code>tf.reshape(x, [batch_size, -1])</code></p> </li> </ul>"},{"location":"chapter3/#pytorch-implementation","title":"PyTorch Implementation","text":"<p>Here\u2019s a full input-to-model example:</p> View Pytorch implementation <pre><code>from PIL import Image\nimport torch\nimport torchvision.transforms as T\nimport torch.nn as nn\n\n# Load and preprocess\nimage = Image.open(\"dog.png\").convert(\"RGB\")\ntransform = T.Compose([\n    T.Resize((224, 224)),\n    T.ToTensor(),  # Converts [H,W,C] to [C,H,W] and scales to 0\u20131\n    T.Normalize(mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225])\n])\ninput_tensor = transform(image).unsqueeze(0)  # [1, 3, 224, 224]\n\n# Example model layer\nconv = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3)\noutput = conv(input_tensor)  # Output shape: [1, 16, 222, 222]\n</code></pre>      If feeding into a dense layer later:     <pre><code>flattened = output.view(output.size(0), -1)  # Flatten to [batch, features]\nfc = nn.Linear(flattened.size(1), 10)\nlogits = fc(flattened)\n</code></pre>"},{"location":"chapter3/#tensorflow-implementation","title":"TensorFlow Implementation","text":"<p>Same input pipeline in TensorFlow: <pre><code>import tensorflow as tf\n\n# Load and preprocess\nimage = tf.io.read_file(\"dog.png\")\nimage = tf.image.decode_jpeg(image, channels=3)\nimage = tf.image.resize(image, [224, 224])\nimage = tf.cast(image, tf.float32) / 255.0\n\n# Normalize with ImageNet stats\nmean = tf.constant([0.485, 0.456, 0.406])\nstd = tf.constant([0.229, 0.224, 0.225])\nimage = (image - mean) / std\n\n# Add batch dimension: [1, 224, 224, 3]\ninput_tensor = tf.expand_dims(image, axis=0)\n\n# Conv2D example\nconv = tf.keras.layers.Conv2D(16, 3)\noutput = conv(input_tensor)  # [1, 222, 222, 16]\n\n# Flatten + Dense\nflattened = tf.reshape(output, [1, -1])\ndense = tf.keras.layers.Dense(10)\nlogits = dense(flattened)\n</code></pre></p>"},{"location":"chapter3/#framework-comparison-table","title":"Framework Comparison Table","text":"Pipeline Step PyTorch TensorFlow Load image Image.open().convert(\"RGB\") tf.io.read_file() + tf.image.decode_*() Resize T.Resize((H, W)) tf.image.resize() Convert to tensor T.ToTensor() tf.cast(..., tf.float32) + divide Normalize T.Normalize(mean, std) Manual: (image - mean) / std Batch dimension tensor.unsqueeze(0) tf.expand_dims(tensor, axis=0) CNN input shape [N, C, H, W] [N, H, W, C] Flatten for Dense .view(N, -1) tf.reshape(..., [N, -1])"},{"location":"chapter3/#mini-exercise","title":"Mini-Exercise","text":"<p>Objective: Build a complete image \u2192 tensor \u2192 CNN pipeline.</p> <ul> <li> <p>Choose any image and ensure it\u2019s RGB.</p> </li> <li> <p>Load and resize it to 224\u00d7224.</p> </li> <li> <p>Normalize using ImageNet mean and std.</p> </li> <li> <p>Add batch dimension and print final shape.</p> </li> <li> <p>Feed into a Conv2D layer and flatten it.</p> </li> <li> <p>Visualize the shape before and after each step.</p> </li> </ul> <p>Bonus Challenge:</p> <ul> <li> <p>Try with grayscale and handle single channel</p> </li> <li> <p>Try with a batch of 5 images</p> </li> </ul>"},{"location":"chapter4/","title":"Chapter 4: Standard Image Preprocessing","text":"<p>\u201cA well-preprocessed image is half the training. The other half is just optimization.\u201d</p>"},{"location":"chapter4/#why-this-chapter-matters","title":"Why This Chapter Matters","text":"<p>Imagine handing a blurry, off-centered photo to a human and asking, \u201cWhat\u2019s in this?\u201d You\u2019d expect a confused answer\u2014and that\u2019s exactly how neural networks feel when they receive poorly scaled, misaligned, or inconsistent input images.</p> <p>Preprocessing is the discipline of preparing your data in a way the model can truly understand. It\u2019s not just resizing or flipping\u2014it\u2019s about standardizing, cleaning, and augmenting images so your CNN can extract meaningful patterns.</p> <p>Done right, preprocessing:</p> <ul> <li> <p>Boosts model accuracy</p> </li> <li> <p>Reduces overfitting</p> </li> <li> <p>Makes inference stable</p> </li> <li> <p>Speeds up convergence</p> </li> </ul> <p>Done wrong? You\u2019ll spend weeks tuning your architecture for nothing.</p>"},{"location":"chapter4/#conceptual-breakdown","title":"Conceptual Breakdown","text":"<p>\ud83d\udd39 What Is Image Preprocessing?</p> <p>Image preprocessing refers to the transformations applied to raw image data before it\u2019s fed into the model. It\u2019s the very first thing your pipeline does, and its job is to:</p> <ul> <li> <p>Resize or crop the image to fit the model\u2019s input size</p> </li> <li> <p>Convert the pixel values to float and normalize them</p> </li> <li> <p>Augment images with randomness during training to improve generalization</p> </li> </ul> <p>\ud83d\udd39 Common Preprocessing Operations</p> Step Purpose Resize Match input shape expected by CNN (e.g., 224\u00d7224) Crop Focus on central content or apply randomness Normalize Scale values for model stability and consistency Augment Random changes (flip, rotate, jitter) to generalize better <p>\ud83d\udd39 Mean-Std Normalization vs 0\u20131 Scaling</p> <p>You\u2019ll often see two types of normalization:</p> <ol> <li> <p>0\u20131 scaling: divide pixel values by 255</p> <ul> <li>Simpler, used for custom models</li> </ul> </li> <li> <p>Mean-std normalization: subtract dataset mean, divide by std</p> <ul> <li>Used with pretrained models (e.g., ResNet, MobileNet)</li> </ul> </li> </ol> Format Example 0\u20131 Scaling <code>img = img / 255.0</code> Mean-std Norm <code>img = (img - mean) / std</code> ImageNet Mean <code>[0.485, 0.456, 0.406]</code> ImageNet Std <code>[0.229, 0.224, 0.225]</code> <p>\ud83d\udd39 Effects of Preprocessing on Training</p> Preprocessing Problem Symptoms No normalization Model fails to converge Wrong mean/std Bad predictions, poor transfer Mismatched resize Shape errors at input layers Augmenting test data Erratic evaluation accuracy <p>Note: Always apply augmentation to training data only, never to validation/test.</p> <p>\ud83d\udd39 PIL vs OpenCV vs tf.image</p> Library Style Format Used PIL Pythonic RGB (default) OpenCV Fast, C++ BGR (must convert!) tf.image Tensor-native TensorFlow Tensors <p>If you\u2019re mixing OpenCV with TensorFlow or PyTorch, be careful\u2014color channels will be flipped unless you convert BGR \u2192 RGB.</p> <p>\ud83d\udd39 Preprocessing Matching: Train vs Inference Your model learns with a certain set of expectations (shape, scale, mean/std). If your inference pipeline doesn\u2019t match your training pipeline, your model will:</p> <p>Output low-confidence predictions</p> <p>Misclassify even familiar data</p> <p>\ud83d\udccc Golden Rule: Always reuse your training preprocessing (minus augmentation) for inference.</p>"},{"location":"chapter4/#pytorch-implementation","title":"PyTorch Implementation","text":"<p>\ud83d\udd38 Training Preprocessing <pre><code>from torchvision import transforms\n\ntrain_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.RandomCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),  # scales to [0, 1]\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n</code></pre></p> <p>\ud83d\udd38 Validation / Inference Preprocessing <pre><code>val_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n</code></pre></p>"},{"location":"chapter4/#tensorflow-implementation","title":"TensorFlow Implementation","text":"<p>\ud83d\udd38 Training Preprocessing <pre><code>import tensorflow as tf\n\ndef preprocess_train(img):\n    img = tf.image.resize(img, [256, 256])\n    img = tf.image.random_crop(img, [224, 224, 3])\n    img = tf.image.random_flip_left_right(img)\n    img = tf.cast(img, tf.float32) / 255.0\n    mean = tf.constant([0.485, 0.456, 0.406])\n    std = tf.constant([0.229, 0.224, 0.225])\n    img = (img - mean) / std\n    return img\n</code></pre></p> <p>\ud83d\udd38 Inference Preprocessing <pre><code>def preprocess_eval(img):\n    img = tf.image.resize(img, [224, 224])\n    img = tf.cast(img, tf.float32) / 255.0\n    mean = tf.constant([0.485, 0.456, 0.406])\n    std = tf.constant([0.229, 0.224, 0.225])\n    img = (img - mean) / std\n    return img\n</code></pre></p>"},{"location":"chapter4/#framework-comparison-table","title":"Framework Comparison Table","text":"Task PyTorch TensorFlow Resize transforms.Resize() tf.image.resize() Crop RandomCrop, CenterCrop tf.image.random_crop() Flip RandomHorizontalFlip() tf.image.random_flip_left_right() Normalize transforms.Normalize(mean, std) Manual: (img - mean) / std Convert to tensor transforms.ToTensor() tf.cast(img, tf.float32) / 255.0 Augment only in training Manual via Compose Wrap in @tf.function or dataset map"},{"location":"chapter4/#mini-exercise","title":"Mini-Exercise","text":"<p>Goal: Create a preprocessing function for training and one for inference.</p> <ol> <li> <p>Pick an image of any size (e.g., 500\u00d7300).</p> </li> <li> <p>Apply:</p> <ul> <li> <p>Resize to 256\u00d7256</p> </li> <li> <p>RandomCrop to 224\u00d7224 (training only)</p> </li> <li> <p>Horizontal flip (training only)</p> </li> <li> <p>Normalize using ImageNet mean/std</p> </li> </ul> </li> <li> <p>Compare preprocessed training and inference outputs.</p> </li> <li> <p>Visualize the results using matplotlib.</p> </li> </ol> <p>Bonus: Try using OpenCV to load the image and manually convert from BGR to RGB.</p>"},{"location":"chapter5/","title":"Chapter 5: Preprocessing for Pretrained Models","text":"<p>\u201cYou can\u2019t transfer knowledge if the inputs don\u2019t speak the same language. Preprocessing isn\u2019t optional\u2014it\u2019s the protocol.\u201d</p>"},{"location":"chapter5/#why-this-chapter-matters","title":"Why This Chapter Matters","text":"<p>Pretrained CNNs\u2014like those from torchvision.models or keras.applications\u2014have been trained on massive datasets like ImageNet. But they were trained with specific input formats in mind:</p> <ul> <li> <p>A fixed input shape (often 224\u00d7224)</p> </li> <li> <p>Normalized using specific mean and std</p> </li> <li> <p>RGB ordering (not grayscale or BGR)</p> </li> <li> <p>Scaled values in a specific range (e.g., [0, 1], [-1, 1])</p> </li> </ul> <p>If you deviate from this, even slightly, your fine-tuned model might:</p> <ul> <li> <p>Output garbage predictions</p> </li> <li> <p>Fail to converge during training</p> </li> <li> <p>Seem to overfit instantly</p> </li> </ul> <p>This chapter teaches you how to match preprocessing exactly to each pretrained model so you can extract their full power\u2014safely.</p>"},{"location":"chapter5/#conceptual-breakdown","title":"Conceptual Breakdown","text":"<p>\ud83d\udd39 Pretrained Model Expectations</p> Model Expected Input Shape Pixel Range Normalization ResNet, VGG 224\u00d7224\u00d73 [0.0 \u2013 1.0] Mean: <code>[0.485, 0.456, 0.406]</code>  Std: <code>[0.229, 0.224, 0.225]</code> MobileNetV2 224\u00d7224\u00d73 [-1.0 \u2013 1.0] <code>preprocess_input()</code> scales it EfficientNet 224\u00d7224\u00d73 or 240\u00d7240 [0.0 \u2013 255.0] <code>preprocess_input()</code> handles it <p>Rule of thumb: Always check the docs or source code of the model you\u2019re using.</p> <p>\ud83d\udd39 Why You Can\u2019t Just Use .ToTensor() or /255.0</p> <p>Because pretrained models were trained on data that was already:</p> <ul> <li> <p>Normalized using dataset-wide statistics</p> </li> <li> <p>Possibly scaled to [-1, 1] or whitened</p> </li> <li> <p>Fed in a specific channel order</p> </li> </ul> <p>If you skip or mismatch the normalization, you're effectively corrupting the input distribution\u2014and the model\u2019s learned filters won\u2019t match.</p> <p>\ud83d\udd39 Matching PyTorch Pretrained Models</p> <p>Most PyTorch models in <code>torchvision.models</code> use:</p> <ul> <li> <p><code>[0, 1]</code> range from <code>ToTensor()</code></p> </li> <li> <p>Mean-std normalization with ImageNet stats</p> </li> </ul> <pre><code>  transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                     std=[0.229, 0.224, 0.225])\n</code></pre> <p>\ud83d\udccc Always use this transform after <code>ToTensor()</code>.</p> <p>\ud83d\udd39 Matching TensorFlow/Keras Pretrained Models</p> <p>Each model has a helper function:</p> Model Family Preprocessing Function ResNet, VGG <code>keras.applications.resnet50.preprocess_input()</code> MobileNetV2 <code>keras.applications.mobilenet_v2.preprocess_input()</code> EfficientNetB0 <code>keras.applications.efficientnet.preprocess_input()</code> <p>These handle:</p> <ul> <li> <p>Mean/std normalization</p> </li> <li> <p>Scaling to [-1, 1] if needed</p> </li> <li> <p>RGB channel order</p> </li> </ul> <p>\ud83d\udccc These functions expect raw pixel values (0\u2013255 float), not normalized.</p>"},{"location":"chapter5/#pytorch-implementation","title":"PyTorch Implementation","text":"<pre><code>from torchvision import models, transforms\nfrom PIL import Image\n\n# Load pretrained ResNet\nmodel = models.resnet50(pretrained=True)\nmodel.eval()\n\n# Preprocessing pipeline\npreprocess = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),  # scales to [0,1]\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\n# Load image and preprocess\nimg = Image.open(\"dog.jpg\").convert(\"RGB\")\ntensor = preprocess(img).unsqueeze(0)  # shape: [1, 3, 224, 224]\n</code></pre>"},{"location":"chapter5/#tensorflow-implementation","title":"TensorFlow Implementation","text":"<pre><code>from tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input\nfrom tensorflow.keras.preprocessing import image\nimport numpy as np\n\n# Load pretrained MobileNetV2\nmodel = MobileNetV2(weights=\"imagenet\")\n\n# Load and preprocess\nimg = image.load_img(\"dog.jpg\", target_size=(224, 224))\nimg_array = image.img_to_array(img)  # shape: [224, 224, 3]\nimg_array = np.expand_dims(img_array, axis=0)  # [1, 224, 224, 3]\nimg_array = preprocess_input(img_array)  # scales to [-1, 1]\n</code></pre>"},{"location":"chapter5/#framework-comparison-table","title":"Framework Comparison Table","text":"Framework Step PyTorch TensorFlow Preprocessing Built-in model normalization transforms.Normalize() keras.applications.*.preprocess_input() Channel Order Input Format [C, H, W] [H, W, C] Expected Range Pixel Values [0, 1] + mean/std [0, 255] \u2192 auto-scaled internally Input Shape Default for pretrained [1, 3, 224, 224] [1, 224, 224, 3]"},{"location":"chapter5/#mini-exercise","title":"Mini-Exercise","text":"<p>Try loading the same image in both PyTorch and TensorFlow. Use:</p> <ul> <li> <p><code>resnet50</code> in PyTorch</p> </li> <li> <p><code>ResNet50</code> in Keras</p> </li> <li> <p>Apply the correct preprocessing for each framework.</p> </li> <li> <p>Feed the tensor into the model and extract the top-1 class prediction.</p> </li> <li> <p>Confirm both models give similar results.</p> </li> </ul> <p>Bonus: Print the difference between preprocessed tensors in PyTorch vs TensorFlow (after matching shape order).</p>"},{"location":"chapter6/","title":"Chapter 6: Image Datasets \u2013 Getting Data Into the Network","text":"<p>\u201cIt\u2019s not just about images\u2014it\u2019s about structure, batching, and flow. A CNN needs data, not chaos.\u201d</p>"},{"location":"chapter6/#why-this-chapter-matters","title":"Why This Chapter Matters","text":"<p>You\u2019ve learned how to preprocess a single image, but deep learning thrives on batches of thousands of images\u2014efficiently streamed, augmented, shuffled, and labeled.</p> <p>If your dataset isn\u2019t:</p> <ul> <li> <p>Properly structured</p> </li> <li> <p>Mapped to class labels</p> </li> <li> <p>Efficiently loaded and shuffled</p> </li> </ul> <p>\u2026 then even the best model won't help you. You\u2019ll face:</p> <ul> <li> <p>GPU idle time</p> </li> <li> <p>Overfitting from data leaks</p> </li> <li> <p>Mislabeling issues</p> </li> </ul> <p>This chapter shows how to transform folders of images into usable model inputs, with complete control over batching, shuffling, label handling, and augmentation.</p>"},{"location":"chapter6/#conceptual-breakdown","title":"Conceptual Breakdown","text":"<p>\ud83d\udd39 The Canonical Folder Structure</p> <p>Most frameworks expect images arranged like this: <pre><code>dataset/\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 class1/\n\u2502   \u2502   \u251c\u2500\u2500 img1.jpg\n\u2502   \u2502   \u251c\u2500\u2500 img2.jpg\n\u2502   \u251c\u2500\u2500 class2/\n\u2502   \u2502   \u251c\u2500\u2500 img3.jpg\n\u2502   \u2502   \u251c\u2500\u2500 img4.jpg\n\u251c\u2500\u2500 val/\n\u2502   \u251c\u2500\u2500 class1/\n\u2502   \u251c\u2500\u2500 class2/\n</code></pre> Each folder name becomes the class label, and all images inside are automatically mapped.</p> <p>This structure is:</p> <ul> <li> <p>Easy to understand</p> </li> <li> <p>Supported natively in both PyTorch and TensorFlow</p> </li> <li> <p>Flexible for augmentation and splitting</p> </li> </ul> <p>\ud83d\udd39 Dataset Terms You Must Know</p> Term Definition Dataset Collection of images and labels (can be lazy-loaded) Dataloader Handles batching, shuffling, and parallel loading Transform Applied per sample to augment/normalize Batch A group of N samples fed to the model at once Epoch One full pass through the dataset <p>\ud83d\udd39 Why Shuffling Matters</p> <p>Shuffling prevents:</p> <ul> <li> <p>Learning label order bias (e.g., all cats, then all dogs)</p> </li> <li> <p>Memorizing batch patterns</p> </li> <li> <p>Overfitting on dataset structure</p> </li> </ul> <p>\ud83d\udccc Always shuffle during training, not validation.</p> <p>\ud83d\udd39 Visualizing a Batch</p> <p>Understanding shapes is critical.</p> Description PyTorch Shape TensorFlow Shape One RGB image [3, H, W] [H, W, 3] One batch (N images) [N, 3, H, W] [N, H, W, 3] One batch + label (images, labels) (images, labels)"},{"location":"chapter6/#pytorch-implementation","title":"PyTorch Implementation","text":"<p>\ud83d\udd38 Folder-based Dataset Loading</p> <pre><code>from torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n# Define transforms\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                         [0.229, 0.224, 0.225])\n])\n\n# Load dataset\ntrain_dataset = datasets.ImageFolder(\"data/train\", transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n# Check class mapping\nprint(train_dataset.class_to_idx)  # e.g., {'cat': 0, 'dog': 1}\n</code></pre> <p>\ud83d\udd38 Iterating Through a Batch</p> <pre><code>for images, labels in train_loader:\n    print(images.shape)  # [32, 3, 224, 224]\n    print(labels)        # Tensor of shape [32]\n    break\n</code></pre>"},{"location":"chapter6/#tensorflow-implementation","title":"TensorFlow Implementation","text":"<p>\ud83d\udd38 Load from Folder with <code>image_dataset_from_directory</code></p> <pre><code>import tensorflow as tf\n\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    \"data/train\",\n    image_size=(224, 224),\n    batch_size=32,\n    label_mode=\"int\",  # or 'categorical' for one-hot\n    shuffle=True\n)\n\n# Check output\nfor images, labels in train_ds.take(1):\n    print(images.shape)  # (32, 224, 224, 3)\n    print(labels.shape)  # (32,)\n</code></pre> <p>\ud83d\udd38 Apply Preprocessing via <code>.map()</code></p> <pre><code>def preprocess(img, label):\n    img = tf.cast(img, tf.float32) / 255.0\n    mean = tf.constant([0.485, 0.456, 0.406])\n    std = tf.constant([0.229, 0.224, 0.225])\n    img = (img - mean) / std\n    return img, label\n\ntrain_ds = train_ds.map(preprocess)\n</code></pre>"},{"location":"chapter6/#framework-comparison-table","title":"Framework Comparison Table","text":"Feature PyTorch TensorFlow Dataset Loader <code>datasets.ImageFolder</code> <code>tf.keras.preprocessing.image_dataset_from_directory</code> Custom Transforms <code>transforms.Compose([...])</code> <code>.map(preprocess_fn)</code> Shuffling <code>shuffle=True</code> in DataLoader <code>shuffle=True</code> in loader or <code>.shuffle(buffer)</code> Label format <code>class_to_idx</code> dictionary Auto-mapped, label mode configurable Image shape in batch <code>[B, 3, H, W]</code> <code>[B, H, W, 3]</code>"},{"location":"chapter6/#mini-exercise","title":"Mini-Exercise","text":"<p>Prepare a small dataset with 2\u20133 image classes (e.g., cat, dog, bird). Then:</p> <ol> <li> <p>Use both PyTorch and TensorFlow to:</p> <ul> <li> <p>Load dataset from folder</p> </li> <li> <p>Apply resizing and normalization</p> </li> <li> <p>Create dataloaders/batched datasets</p> </li> </ul> </li> <li> <p>Iterate through 1 batch and print:</p> <ul> <li> <p>Image tensor shape</p> </li> <li> <p>Label mapping (class-to-index)</p> </li> </ul> </li> <li> <p>Try visualizing one batch using matplotlib.</p> <ul> <li>Bonus: Add a simple augmentation (flip or crop) and re-inspect the batch output.</li> </ul> </li> </ol>"},{"location":"chapter7/","title":"Chapter 7: Data Augmentation Techniques (Expanded)","text":"<p>\u201cIf your model memorizes your dataset, you\u2019ve failed. Augmentation teaches it to imagine.\u201d</p>"},{"location":"chapter7/#why-this-chapter-matters","title":"Why This Chapter Matters","text":"<p>Most real-world datasets are:</p> <ul> <li> <p>Small</p> </li> <li> <p>Biased</p> </li> <li> <p>Repetitive</p> </li> </ul> <p>Without augmentation, your CNN learns to memorize patterns instead of generalizing. That\u2019s why data augmentation is not just a \u201cnice to have\u201d\u2014it\u2019s a core strategy to help models perform better on unseen data.</p> <p>In this chapter, we go beyond the basics:</p> <ul> <li> <p>You\u2019ll learn classic augmentations like random crop, flip, and jitter</p> </li> <li> <p>Then expand into modern techniques like Cutout, Mixup, and CutMix</p> </li> <li> <p>And you\u2019ll implement these in both PyTorch and TensorFlow, with visualization</p> </li> </ul>"},{"location":"chapter7/#conceptual-breakdown","title":"Conceptual Breakdown","text":"<p>\ud83d\udd39 What is Data Augmentation?</p> <p>Augmentation is the process of applying random transformations to training images on the fly\u2014so the model sees a new version of each image every epoch.</p> <p>It\u2019s only used during training, never during validation or inference.</p> <p>\ud83d\udd39 Classic Augmentations</p> Augmentation Effect RandomCrop Focus on subregions, simulate framing variation HorizontalFlip Simulate left-right symmetry ColorJitter Adjust brightness, contrast, saturation, hue Rotation Handle orientation bias Gaussian Blur Simulate camera focus variation <p>\ud83d\udd39 Advanced Augmentations</p> Technique Description Cutout Randomly removes a square region (forces model to focus elsewhere) Mixup Blends two images and their labels linearly CutMix Combines patches from different images (and labels) <p>\ud83d\udd39 Why They Work</p> <ul> <li> <p>Cutout teaches robustness to occlusion</p> </li> <li> <p>Mixup teaches interpolation between classes</p> </li> <li> <p>CutMix teaches spatial composition and label smoothing</p> </li> </ul> <p>\ud83d\udccc These augmentations improve generalization, reduce overfitting, and even improve model calibration.</p>"},{"location":"chapter7/#pytorch-implementation","title":"PyTorch Implementation","text":"<p>\ud83d\udd38 Classic Augmentations</p> <pre><code>from torchvision import transforms\n\ntrain_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.RandomCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                         [0.229, 0.224, 0.225])\n])\n</code></pre> <p>\ud83d\udd38 Cutout (Custom)</p> <pre><code>import torch\nimport numpy as np\nimport torchvision.transforms.functional as F\n\nclass Cutout(object):\n    def __init__(self, size=50):\n        self.size = size\n\n    def __call__(self, img):\n        c, h, w = img.shape\n        y = np.random.randint(h)\n        x = np.random.randint(w)\n\n        y1 = np.clip(y - self.size // 2, 0, h)\n        y2 = np.clip(y + self.size // 2, 0, h)\n        x1 = np.clip(x - self.size // 2, 0, w)\n        x2 = np.clip(x + self.size // 2, 0, w)\n\n        img[:, y1:y2, x1:x2] = 0.0\n        return img\n</code></pre> <p>Add it to your transform:</p> <pre><code>train_transform.transforms.append(Cutout(size=32))\n</code></pre>"},{"location":"chapter7/#tensorflow-implementation","title":"TensorFlow Implementation","text":"<p>\ud83d\udd38 Classic Augmentations</p> <p><pre><code>import tensorflow as tf\nfrom tensorflow.keras import layers\n\ndata_augment = tf.keras.Sequential([\n    layers.Resizing(256, 256),\n    layers.RandomCrop(224, 224),\n    layers.RandomFlip(\"horizontal\"),\n    layers.RandomBrightness(factor=0.2),\n    layers.RandomContrast(factor=0.2),\n])\n</code></pre> Use during training: <pre><code>train_ds = train_ds.map(lambda x, y: (data_augment(x), y))\n</code></pre></p> <p>\ud83d\udd38 CutMix</p> <pre><code>import tensorflow_addons as tfa\n\ndef cutmix(images, labels, alpha=1.0):\n    batch_size = tf.shape(images)[0]\n    indices = tf.random.shuffle(tf.range(batch_size))\n    shuffled_images = tf.gather(images, indices)\n    shuffled_labels = tf.gather(labels, indices)\n\n    lam = tf.random.uniform([], 0, 1)\n    image_shape = tf.shape(images)[1:]\n    cut_w = tf.cast(image_shape[1] * tf.math.sqrt(1 - lam), tf.int32)\n    cut_h = tf.cast(image_shape[0] * tf.math.sqrt(1 - lam), tf.int32)\n\n    cx = tf.random.uniform([], 0, image_shape[1], dtype=tf.int32)\n    cy = tf.random.uniform([], 0, image_shape[0], dtype=tf.int32)\n\n    x1 = tf.clip_by_value(cx - cut_w // 2, 0, image_shape[1])\n    y1 = tf.clip_by_value(cy - cut_h // 2, 0, image_shape[0])\n    x2 = tf.clip_by_value(cx + cut_w // 2, 0, image_shape[1])\n    y2 = tf.clip_by_value(cy + cut_h // 2, 0, image_shape[0])\n\n    padding = [[0, 0], [y1, image_shape[0] - y2], [x1, image_shape[1] - x2], [0, 0]]\n    cutmix_img = tf.pad(shuffled_images, padding, constant_values=0)\n\n    new_images = tf.tensor_scatter_nd_update(images, [[0]], [cutmix_img])\n    new_labels = lam * labels + (1 - lam) * shuffled_labels\n\n    return new_images, new_labels\n</code></pre> <p>\ud83d\udccc You can also use TensorFlow Addons or Albumentations for more advanced pipelines.</p>"},{"location":"chapter7/#framework-comparison-table","title":"Framework Comparison Table","text":"Augmentation PyTorch (torchvision) TensorFlow (Keras or tf.data) Resize/Crop/Flip <code>transforms.*</code> <code>layers.Resizing()</code>, <code>layers.Random*()</code> Color Jitter <code>transforms.ColorJitter()</code> <code>layers.RandomBrightness()</code>, etc. Cutout Custom class Custom or <code>tfa.image.random_cutout()</code> Mixup Custom function Custom function or <code>tf.image</code> logic CutMix Custom function TensorFlow Addons or custom logic Batch-safe usage <code>transforms.Compose()</code> + DataLoader <code>.map(lambda x, y: ...)</code> in <code>tf.data</code>"},{"location":"chapter7/#mini-exercise","title":"Mini-Exercise","text":"<ol> <li> <p>Pick a sample dataset (e.g., 100 dog and cat images)</p> </li> <li> <p>Apply:</p> </li> </ol> <p>\u00a0\u00a0\u00a0\u00a0 \ud83d\udd38 RandomCrop + Flip + ColorJitter (PyTorch)</p> <p>\u00a0\u00a0\u00a0\u00a0 \ud83d\udd38 Resize + RandomBrightness (TF)</p> <ol> <li>Implement:</li> </ol> <p>\u00a0\u00a0\u00a0\u00a0 \ud83d\udd38 Cutout in PyTorch</p> <p>\u00a0\u00a0\u00a0\u00a0 \ud83d\udd38 Mixup or CutMix in TensorFlow</p> <ol> <li> <p>Visualize 5 examples before and after augmentation</p> </li> <li> <p>Train a simple CNN with and without augmentation\u2014observe accuracy</p> </li> </ol>"},{"location":"chapter8/","title":"Chapter 8: Understanding CNN Layers","text":"<p>\u201cEvery filter is a lens. Every layer is a language. A CNN doesn\u2019t just see\u2014it interprets.\u201d</p>"},{"location":"chapter8/#why-this-chapter-matters","title":"Why This Chapter Matters","text":"<p>A Convolutional Neural Network is more than a stack of layers\u2014it\u2019s a hierarchy of abstractions. With each convolution, pooling, and activation, your model goes from low-level pixels to high-level semantics:</p> <ul> <li>Edge \u2192 Shape \u2192 Texture \u2192 Object</li> </ul> <p>But to design effective CNNs (and debug them), you need to understand how each layer transforms the input.</p> <p>This chapter walks you through:</p> <ul> <li>What each major CNN layer does</li> <li>How it changes shape, depth, and meaning</li> <li>How to implement and visualize these layers in PyTorch and TensorFlow</li> </ul> <p>You\u2019ll finally understand why a 224\u00d7224\u00d73 image turns into a 7\u00d77\u00d7512 feature map.</p>"},{"location":"chapter8/#conceptual-breakdown","title":"Conceptual Breakdown","text":""},{"location":"chapter8/#the-core-cnn-layer-types","title":"\ud83d\udd39 The Core CNN Layer Types","text":"Layer Function Conv2D Applies a filter/kernel over spatial regions Activation (ReLU) Adds non-linearity so the network can learn complex patterns BatchNorm Normalizes activations to stabilize training Pooling Reduces spatial size while keeping key features Dropout Prevents overfitting by randomly dropping activations Fully Connected Maps final features to output classes"},{"location":"chapter8/#convolution-layer-conv2d","title":"\ud83d\udd39 Convolution Layer: <code>Conv2D</code>","text":"<ul> <li>Uses a kernel (e.g., 3\u00d73) that slides across the image</li> <li>Performs element-wise multiplications and adds up the result</li> <li>Outputs a feature map</li> </ul> <p>\ud83d\udccc A convolution layer doesn\u2019t see the entire image\u2014it sees a window. As we stack layers, the receptive field grows.</p> <p>Key parameters:</p> <ul> <li><code>in_channels</code>: number of input feature channels</li> <li><code>out_channels</code>: number of filters (i.e., output channels)</li> <li><code>kernel_size</code>: size of each filter (e.g., 3\u00d73)</li> <li><code>stride</code>: how much the filter moves per step</li> <li><code>padding</code>: how edges are handled (valid vs same)</li> </ul>"},{"location":"chapter8/#pooling-layer-maxpool2d-avgpool2d","title":"\ud83d\udd39 Pooling Layer: <code>MaxPool2D</code>, <code>AvgPool2D</code>","text":"<ul> <li>Downsamples feature maps (e.g., from 32\u00d732 \u2192 16\u00d716)</li> <li>Keeps strongest signals (MaxPooling) or averages regions (AvgPooling)</li> <li>Reduces computation and helps detect patterns invariant to position</li> </ul>"},{"location":"chapter8/#batch-normalization","title":"\ud83d\udd39 Batch Normalization","text":"<ul> <li>Normalizes output of a layer to have zero mean, unit variance</li> <li>Stabilizes training, allows for higher learning rates</li> <li>Applied after convolution, before activation</li> </ul>"},{"location":"chapter8/#activation-functions-relu-and-beyond","title":"\ud83d\udd39 Activation Functions: ReLU and Beyond","text":"Activation Formula Purpose ReLU <code>max(0, x)</code> Introduces non-linearity Leaky ReLU <code>max(\u03b1x, x)</code> Keeps small negative slope Sigmoid <code>1 / (1 + e^-x)</code> Squeezes to [0, 1] <p>\ud83d\udccc Most modern CNNs use ReLU for its simplicity and efficiency.</p>"},{"location":"chapter8/#fully-connected-dense-layers","title":"\ud83d\udd39 Fully Connected (Dense) Layers","text":"<p>After several convolution + pooling blocks, the feature map is flattened into a vector and passed through one or more <code>Linear</code> (PyTorch) or <code>Dense</code> (TF) layers.</p> <ul> <li>Used to classify based on the features extracted earlier</li> <li>Last layer\u2019s size = number of classes</li> </ul>"},{"location":"chapter8/#pytorch-implementation","title":"PyTorch Implementation","text":"<p>Let\u2019s build a simple Conv \u2192 ReLU \u2192 Pool block:</p> <pre><code>import torch.nn as nn\n\ncnn_block = nn.Sequential(\n    nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),  # [B, 3, 224, 224] \u2192 [B, 16, 224, 224]\n    nn.BatchNorm2d(16),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size=2, stride=2)  # [B, 16, 224, 224] \u2192 [B, 16, 112, 112]\n)\n</code></pre> <p>A full model:</p> <pre><code>class SimpleCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 16, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(16, 32, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(32 * 56 * 56, 10)  # assuming input was 224x224\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        return self.classifier(x)\n</code></pre>"},{"location":"chapter8/#tensorflow-implementation","title":"TensorFlow Implementation","text":"<pre><code>from tensorflow.keras import layers, models\n\nmodel = models.Sequential([\n    layers.Conv2D(16, (3, 3), padding='same', input_shape=(224, 224, 3)),\n    layers.BatchNormalization(),\n    layers.ReLU(),\n    layers.MaxPooling2D(pool_size=(2, 2)),\n\n    layers.Conv2D(32, (3, 3), padding='same'),\n    layers.ReLU(),\n    layers.MaxPooling2D(pool_size=(2, 2)),\n\n    layers.Flatten(),\n    layers.Dense(10)\n])\n</code></pre>"},{"location":"chapter8/#how-shapes-change","title":"How Shapes Change","text":"Operation PyTorch Shape Change TensorFlow Shape Change Conv2D <code>[B, C_in, H, W] \u2192 [B, C_out, H, W]</code> <code>[B, H, W, C_in] \u2192 [B, H, W, C_out]</code> MaxPool2D (2\u00d72) <code>[B, C, H, W] \u2192 [B, C, H/2, W/2]</code> <code>[B, H, W, C] \u2192 [B, H/2, W/2, C]</code> Flatten <code>[B, C, H, W] \u2192 [B, C\u00d7H\u00d7W]</code> <code>[B, H, W, C] \u2192 [B, H\u00d7W\u00d7C]</code>"},{"location":"chapter8/#framework-comparison-table","title":"Framework Comparison Table","text":"Layer PyTorch TensorFlow Convolution <code>nn.Conv2d(in, out, k)</code> <code>layers.Conv2D(filters, k)</code> Pooling <code>nn.MaxPool2d(k)</code> <code>layers.MaxPooling2D(k)</code> BatchNorm <code>nn.BatchNorm2d(channels)</code> <code>layers.BatchNormalization()</code> Activation (ReLU) <code>nn.ReLU()</code> or <code>F.relu()</code> <code>layers.ReLU()</code> or inline Fully Connected <code>nn.Linear(in, out)</code> <code>layers.Dense(units)</code> Flatten <code>nn.Flatten()</code> <code>layers.Flatten()</code>"},{"location":"chapter8/#mini-exercise","title":"Mini-Exercise","text":"<p>Build a mini CNN with:</p> <ul> <li>2 Conv2D layers</li> <li>ReLU and MaxPooling after each</li> <li> <p>Flatten + Dense to output 10 classes</p> </li> <li> <p>Feed a dummy input of shape <code>[1, 3, 224, 224]</code> (PyTorch) or <code>[1, 224, 224, 3]</code> (TF)</p> </li> <li>Print the shape after each layer</li> <li>Try replacing ReLU with LeakyReLU\u2014observe differences</li> </ul> <p>Bonus: Visualize the first convolutional layer filters (we\u2019ll expand this in Chapter 17!)</p>"},{"location":"chapter9/","title":"Chapter 9: The CNN Vocabulary (Terms Demystified)","text":"<p>\u201cBefore you build deep networks, build deep understanding. Words like kernel, stride, and feature map aren\u2019t just jargon\u2014they\u2019re the gears of a vision engine.\u201d</p>"},{"location":"chapter9/#why-this-chapter-matters","title":"Why This Chapter Matters","text":"<p>If you\u2019ve ever wondered:</p> <ul> <li>\u201cWhat exactly is a kernel?\u201d</li> <li>\u201cHow do channels differ from filters?\u201d</li> <li>\u201cWhy does stride affect output shape?\u201d</li> <li>\u201cWhat\u2019s the difference between padding types?\u201d</li> </ul> <p>\u2026 then this chapter is for you.</p> <p>Clear understanding of these terms helps you:</p> <ul> <li>Design architectures confidently</li> <li>Avoid shape mismatch bugs</li> <li>Communicate ideas and debug issues quickly</li> <li>Understand pretrained model behavior</li> </ul>"},{"location":"chapter9/#conceptual-breakdown","title":"Conceptual Breakdown","text":"<p>Let\u2019s define and visually ground each essential CNN term.</p>"},{"location":"chapter9/#kernel-aka-filter","title":"\ud83d\udd39 Kernel (a.k.a. Filter)","text":"<p>What it is: A small matrix (e.g., 3\u00d73 or 5\u00d75) that slides across the image, performing local dot products.</p> <ul> <li>Each kernel learns to detect a pattern (e.g., edge, curve, texture)</li> <li>A Conv2D layer contains many kernels\u2014one per output channel</li> </ul> <p>Think of a kernel as the \"eye\" scanning a small area.</p> Size Meaning 1\u00d71 Channel-wise projection 3\u00d73 Local feature extraction 5\u00d75 More context, costlier"},{"location":"chapter9/#stride","title":"\ud83d\udd39 Stride","text":"<p>What it is: The number of pixels the kernel moves each time.</p> <ul> <li>Stride = 1 \u2192 overlapping windows</li> <li>Stride = 2 \u2192 skips every other pixel, downsamples output</li> </ul> <p>Stride controls spatial resolution of the output.</p>"},{"location":"chapter9/#padding","title":"\ud83d\udd39 Padding","text":"<p>What it is: How we handle the edges of the image.</p> Type Description Valid No padding (output shrinks) Same Pads so output shape matches input (if stride=1) Custom Manually pad with specific values <p>\ud83d\udccc In PyTorch: <code>padding=1</code> for 3\u00d73 kernel maintains shape \ud83d\udccc In TensorFlow: use <code>padding='same'</code> or <code>'valid'</code></p>"},{"location":"chapter9/#inputoutput-channels","title":"\ud83d\udd39 Input/Output Channels","text":"<p>Input Channels: Number of channels in the incoming tensor Output Channels: Number of filters (each outputs a channel)</p> Layer Input Shape Output Shape Conv2D <code>[B, 3, H, W]</code> (RGB) <code>[B, 64, H, W]</code> (64 filters) <p>Every output channel corresponds to one kernel applied across all input channels.</p>"},{"location":"chapter9/#feature-maps","title":"\ud83d\udd39 Feature Maps","text":"<p>What it is: The output of a convolution layer\u2014a 2D activation map showing how strongly a feature was detected in different regions.</p> <ul> <li>Early layers: feature maps detect edges, corners</li> <li>Deeper layers: feature maps detect eyes, wheels, textures</li> </ul> <p>\ud83d\udccc Feature maps = filtered views of the image.</p>"},{"location":"chapter9/#receptive-field","title":"\ud83d\udd39 Receptive Field","text":"<p>What it is: The effective area of the original input that a neuron \u201csees.\u201d</p> <ul> <li>Grows with depth</li> <li>A neuron in a deep layer might \u201csee\u201d the entire image</li> </ul> <p>A large receptive field = global understanding Small receptive field = local detail</p>"},{"location":"chapter9/#channel-depth-vs-spatial-dimensions","title":"\ud83d\udd39 Channel Depth vs Spatial Dimensions","text":"Property Meaning Spatial size Height \u00d7 Width (resolution) Depth Number of feature channels <p>Example: <code>[32, 128, 128]</code> = 32 filters, 128\u00d7128 resolution per map</p>"},{"location":"chapter9/#layer-variants","title":"\ud83d\udd39 Layer Variants","text":"Term Meaning ReflectionPad2d Pads by mirroring the image at the edge (used in style transfer) InstanceNorm2d Like BatchNorm, but per image-instance (used in image generation tasks) AdaptiveAvgPool2d Automatically resizes output to fixed size regardless of input size <p>These are powerful tools when building style-transfer, GANs, or segmentation models.</p>"},{"location":"chapter9/#pytorch-examples","title":"PyTorch Examples","text":"<pre><code>import torch.nn as nn\n\n# 3x3 conv, same output shape\nconv = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n\n# Pooling\npool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n# Adaptive pooling to 1\u00d71 (useful before a Dense layer)\nadaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n\n# Reflection padding (e.g., style transfer)\npad = nn.ReflectionPad2d(2)\n\n# Instance normalization (used in generator networks)\nnorm = nn.InstanceNorm2d(16)\n</code></pre>"},{"location":"chapter9/#tensorflow-examples","title":"TensorFlow Examples","text":"<pre><code>from tensorflow.keras import layers\n\n# Conv with SAME padding\nconv = layers.Conv2D(16, kernel_size=3, padding='same')\n\n# Max Pooling\npool = layers.MaxPooling2D(pool_size=(2, 2), strides=2)\n\n# Adaptive pooling (Global Average Pool)\nadaptive = layers.GlobalAveragePooling2D()\n\n# Reflection padding: must be done manually\npadded = tf.pad(input_tensor, [[0, 0], [2, 2], [2, 2], [0, 0]], mode='REFLECT')\n\n# Instance norm (use tf_addons or custom layer)\n</code></pre>"},{"location":"chapter9/#framework-comparison-table","title":"Framework Comparison Table","text":"Concept PyTorch TensorFlow Conv2D <code>nn.Conv2d(in, out, k)</code> <code>layers.Conv2D(out, k, padding=...)</code> Padding (same) <code>padding=1</code> (for 3\u00d73) <code>padding='same'</code> Adaptive pooling <code>AdaptiveAvgPool2d(output_size)</code> <code>GlobalAveragePooling2D()</code> InstanceNorm <code>nn.InstanceNorm2d()</code> Addons/custom implementation Reflection padding <code>nn.ReflectionPad2d(pad)</code> <code>tf.pad(..., mode='REFLECT')</code>"},{"location":"chapter9/#mini-exercise","title":"Mini-Exercise","text":"<p>Choose an image and:</p> <ol> <li> <p>Manually implement:</p> </li> <li> <p>A 3\u00d73 Conv2D with stride 1 and padding 1</p> </li> <li>A MaxPool2D with stride 2</li> <li> <p>A GlobalAveragePooling layer</p> </li> <li> <p>Print the shape of each output step-by-step</p> </li> <li> <p>Visualize:</p> </li> <li> <p>The input</p> </li> <li>The output feature maps of the first convolution</li> </ol> <p>Bonus: Try using <code>AdaptiveAvgPool2d((1, 1))</code> to make your model input-shape agnostic.</p>"}]}